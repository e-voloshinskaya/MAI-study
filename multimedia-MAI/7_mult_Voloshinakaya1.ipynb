{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2saVOMY-mk3w"
      },
      "source": [
        "# Лабораторные работы по дисциплине \"Методы, средства и технологии мультимедиа\"\n",
        "\n",
        "**Выполнила студентка гр. М8О-406Б-21 Волошинская Евгения Владимировна**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAja5slwj-Oi"
      },
      "source": [
        "В качестве данных я выбрала датасет Semantic Segmentation Drone Dataset (https://www.kaggle.com/datasets/santurini/semantic-segmentation-drone-dataset).\n",
        "Эти данные можно использовать для обучения моделей компьютерного зрения, которые автоматически анализируют аэрофотосъёмку с дронов: прокладывают безопасный маршрут, обновляют карты дорог и зданий, оценивают состояние инфраструктуры и выделяют зелёные зоны. Это повышает точность навигации, снижает ручную работу операторов и ускоряет принятие решений в городских службах.\n",
        "\n",
        "**Датасет поддерживает две постановки задачи семантической сегментации:**\n",
        "\n",
        "☑ Бинарная – отделить объекты от фона (быстрый базовый вариант).\n",
        "\n",
        "☐ 5-классная – раскрасить каждый пиксель по макрогруппам: небо, здания/препятствия, дорога/земля, растительность, транспорт/люди.\n",
        "\n",
        "Была выбрана задача бинарной сегментации, т.к. в реализации проекта используются ограниченные возможности бесплатной версии Google Colab.\n",
        "\n",
        "**Ключевые метрики качества:**\n",
        "\n",
        "• mIoU (mean Intersection over Union)– основной индикатор перекрытия предсказанных и истинных масок,\n",
        "где IoU - отношение пересечения предсказанной и истинной масок к их объединению, mIoU - среднее значение IoU по всем классамю\n",
        "\n",
        "• Dice/F1-score – гармоника точности и полноты по пикселям, особенно важна при бинарной задаче.\n",
        "\n",
        "• Pixel Accuracy – доля правильно размеченных пикселей (вспомогательная).\n",
        "\n",
        "IoU и Dice по каждому классу – показывают, не «теряются» ли редкие объекты."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Структура датасета"
      ],
      "metadata": {
        "id": "T7lCX8RXOQlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В основной папке датасета находится папка binary_dataset/binary_dataset, которая содержит две подпапки: с оригинальными изображениями (original_images) и с размеченными данными (images_semantic)\n",
        "```\n",
        "binary_dataset/\n",
        "├── original_images/\n",
        "│   ├── 000.png\n",
        "│   └── 001.png\n",
        "└── images_semantic/\n",
        "    ├── 000.png\n",
        "    └── 001.png\n",
        "```"
      ],
      "metadata": {
        "id": "XQf1U_xbOPQZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M14zCixiU4qU"
      },
      "source": [
        "## Лабораторная работа №7: Проведение исследований моделями семантической сегментации"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"santurini/semantic-segmentation-drone-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovPl4g6CnYMN",
        "outputId": "2052bd1c-bd0b-47a2-b658-e0e00a171c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/semantic-segmentation-drone-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade pip\n",
        "!pip install -q pytorch-lightning segmentation_models_pytorch albumentations timm tqdm"
      ],
      "metadata": {
        "id": "uxeMHsxwzshj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7090ba-c876-447e-ae2c-3b577b74b537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m187.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m187.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m206.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [segmentation_models_pytorch]\n",
            "\u001b[1A\u001b[2K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Бейзлайн"
      ],
      "metadata": {
        "id": "0JFJa5KyBElO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве бейзлайна были обучены две модели:\n",
        "\n",
        "1. Сверточная модель U-Net с энкодером ResNet34;\n",
        "\n",
        "2. DeepLabV3+ с трансформерным энкодером MIT-B0.\n",
        "\n",
        "Оценка производилась по метрикам IoU (Intersection over Union), Accuracy и Dice Loss на тестовой выборке."
      ],
      "metadata": {
        "id": "zM-mU_DlN5R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 1. Подключение библиотек\n",
        "# ================================================================\n",
        "import os, random, cv2, numpy as np, torch, pytorch_lightning as pl\n",
        "from pathlib import Path\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import segmentation_models_pytorch as smp\n",
        "import inspect\n",
        "\n",
        "USE_WANDB = False\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# ================================================================\n",
        "# 2. Пути к папкам с файлами\n",
        "# ================================================================\n",
        "ROOT = Path(\"/kaggle/input/semantic-segmentation-drone-dataset/binary_dataset/binary_dataset\")\n",
        "IMG_DIR  = ROOT / \"original_images\"\n",
        "MASK_DIR = ROOT / \"images_semantic\"\n",
        "assert IMG_DIR.exists() and MASK_DIR.exists()\n",
        "\n",
        "# ================================================================\n",
        "# 3. FILES + SPLIT\n",
        "# ================================================================\n",
        "imgs  = sorted(IMG_DIR.glob(\"*.png\"))\n",
        "if not imgs:\n",
        "    raise RuntimeError(\"В original_images нет *.png файлов\")\n",
        "\n",
        "masks = [MASK_DIR / f\"{p.stem}.png\" for p in imgs]\n",
        "for m in masks:\n",
        "    if not m.exists():\n",
        "        raise RuntimeError(f\"Нет маски для {m.stem}.png\")\n",
        "\n",
        "perm  = np.random.permutation(len(imgs))\n",
        "tr, vl = int(.8*len(perm)), int(.9*len(perm))\n",
        "split_files = dict(\n",
        "    train=(np.array(imgs)[perm[:tr]],   np.array(masks)[perm[:tr]]),\n",
        "    val  =(np.array(imgs)[perm[tr:vl]], np.array(masks)[perm[tr:vl]]),\n",
        "    test =(np.array(imgs)[perm[vl:]],   np.array(masks)[perm[vl:]]),\n",
        ")\n",
        "\n",
        "# ================================================================\n",
        "# 4. DATASET\n",
        "# ================================================================\n",
        "class DroneBinDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, imgs, masks, tfm):\n",
        "        self.imgs, self.masks, self.t = list(imgs), list(masks), tfm\n",
        "    def __len__(self):  return len(self.imgs)\n",
        "    def __getitem__(self, i):\n",
        "        img  = cv2.cvtColor(cv2.imread(str(self.imgs[i])), cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(str(self.masks[i]), 0)\n",
        "        mask = (mask > 127).astype(\"float32\")\n",
        "        batch = self.t(image=img, mask=mask)\n",
        "        return batch[\"image\"], batch[\"mask\"].unsqueeze(0)\n",
        "\n",
        "# ================================================================\n",
        "# 5. AUGMENTATIONS\n",
        "# ================================================================\n",
        "train_tf = A.Compose([\n",
        "    A.Resize(736, 736),\n",
        "    A.Normalize(), ToTensorV2()\n",
        "])\n",
        "test_tf  = train_tf\n",
        "\n",
        "def run_baseline():\n",
        "    for name, lr in [(\"Unet\",3e-4), (\"DeepLabV3Plus\",5e-4)]:\n",
        "        trainer = pl.Trainer(max_epochs=15,\n",
        "                             accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                             devices=1, log_every_n_steps=10,\n",
        "                             enable_checkpointing=False)\n",
        "        trainer.fit(LitSeg(name, lr)); trainer.test(LitSeg(name, lr))\n",
        "# ================================================================\n",
        "# 6. LIGHTNING:\n",
        "# PyTorch Lightning — высокоуровневая обёртка над PyTorch, которая позволяет писать более чистый, модульный и удобный для экспериментов код.\n",
        "# ================================================================\n",
        "MODELS = {\n",
        "    \"Unet\": lambda: smp.Unet(\"resnet34\", encoder_weights=\"imagenet\",\n",
        "                             in_channels=3, classes=1, activation=None),\n",
        "    \"DeepLabV3Plus\": lambda: smp.DeepLabV3Plus(\n",
        "        encoder_name=\"mit_b0\", encoder_weights=\"imagenet\",\n",
        "        in_channels=3, classes=1, activation=None),\n",
        "}\n",
        "LOSS = smp.losses.DiceLoss(\"binary\", from_logits=True)\n",
        "\n",
        "class LitSeg(pl.LightningModule):\n",
        "    def __init__(self, name, lr=3e-4, bs=8):\n",
        "        super().__init__()\n",
        "        self.model, self.lr, self.bs = MODELS[name](), lr, bs\n",
        "        self.save_hyperparameters()\n",
        "    # data\n",
        "    def setup(self, stage=None):\n",
        "        self.ds_train = DroneBinDS(*split_files[\"train\"], train_tf)\n",
        "        self.ds_val   = DroneBinDS(*split_files[\"val\"],   test_tf)\n",
        "        self.ds_test  = DroneBinDS(*split_files[\"test\"],  test_tf)\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.ds_train,\n",
        "                                           batch_size=self.bs,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.ds_val,\n",
        "                                           batch_size=self.bs,\n",
        "                                           shuffle=False,\n",
        "                                           num_workers=2)\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.ds_test,\n",
        "                                           batch_size=self.bs,\n",
        "                                           shuffle=False,\n",
        "                                           num_workers=2)\n",
        "    # loop\n",
        "    def shared_step(self,b,tag):\n",
        "        x,y=b\n",
        "        logits=self.model(x); loss=LOSS(logits,y)\n",
        "        pred=(torch.sigmoid(logits)>=.5).long()\n",
        "        tp,fp,fn,tn=smp.metrics.get_stats(pred,y.long(),mode=\"binary\")\n",
        "        iou=smp.metrics.iou_score(tp,fp,fn,tn,reduction=\"micro-imagewise\")\n",
        "        acc=smp.metrics.accuracy(tp,fp,fn,tn,reduction=\"micro\")\n",
        "        self.log_dict({f\"{tag}_loss\":loss,f\"{tag}_iou\":iou,f\"{tag}_acc\":acc},prog_bar=True)\n",
        "        return loss\n",
        "    def training_step(self,b,_):   return self.shared_step(b,\"train\")\n",
        "    def validation_step(self,b,_): return self.shared_step(b,\"val\")\n",
        "    def test_step(self,b,_):       return self.shared_step(b,\"test\")\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "# ================================================================\n",
        "# 7. RUN\n",
        "# ================================================================\n",
        "def run(name, lr):\n",
        "    logger = (pl.loggers.WandbLogger(project=\"seg_drone\", name=name)\n",
        "              if USE_WANDB else None)\n",
        "    model  = LitSeg(name, lr)\n",
        "    trainer = pl.Trainer(accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "                         devices=1, max_epochs=15, logger=logger,\n",
        "                         log_every_n_steps=10, enable_checkpointing=False)\n",
        "    trainer.fit(model); trainer.test(model)\n",
        "\n",
        "run(\"Unet\",          lr=3e-4)   # CNN\n",
        "run(\"DeepLabV3Plus\", lr=5e-4)   # Transformer (mit_b0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "66eb620550e34c98975a5fdf897e8aab",
            "b4ce3f9c3768442bb613433606b5e9fa",
            "72908becf4e9405baefd220983cbcd66",
            "d6eacd48940e4540a2edb45d0f017f8a",
            "f078de8706d343a9a75a6a74cf59afa1",
            "c1d5d49bdd3f4291aa2068ab16da2c76",
            "a1585cf153bb47ca92df12c693770075",
            "5018b5dcde624d1a98e08ce5c39a46d2",
            "d942db6819584116897a11c6e4a94b69",
            "bbfe55cd74d344f2964a5adb687a7120",
            "1274018f1ed74b2a896c1fe98d9fe82f",
            "157e4ceae7984f31949dc52c93fe810d",
            "50f6e67ac4a54afb966541f71c654506",
            "3ac696c68ba2422d8883f65fb2dfdb3a",
            "0c72c7b91d334a91921090e08d596745",
            "d230e9a9b60d4b91b5bf347bfe37c08e",
            "0391454c1fd3432a81472d6996adf732",
            "4ed31f3db224424eb7bd023127060460",
            "0f72fd8d432249bc87b5d1d2e2f4da9f",
            "380863ff90b949ddaf2fe4b53685abbb",
            "f443a9d4519d44a28b8fc6ade728bd54",
            "b1c694b2097b4e34bc23bfb32e94d0cc",
            "eb30e004811544b4aea7961db2e7f90a",
            "9b58411232ed4a6f95dda19e0843ed30",
            "af17de9208c24303bf67ddcf38210cc0",
            "9e4305b928784fef8b6d35922c5c0d21",
            "c6037316181d494e892c8dff83a628e2",
            "6eb69cba315e4f7d95a837b29083ec44",
            "478ba113cf5f44f09487a53b8812c7ee",
            "e45c2aa8a3a2426a8b8bbe481866ea1c",
            "d4c833ce92c743379abb110575cc8ed4",
            "7b99152536984e8b89008a2c22f3e309",
            "c3037e9974af490b9f9fb6a94a89998b",
            "ba6e6f946f0a433d9f16e405a71aba21",
            "0ccee0602de04ce3814ef9a5832d2c55",
            "299d749006db49ea8dea0ecdd0e1c6a7",
            "bada719a93dd4ad398a02cbd0518924b",
            "f516b24237fd4210b741c4edbb2c8526",
            "a5454a24fd214d2295b24b1e146d4974",
            "1d4cefd47a3242ce9331882fcbc2de50",
            "3b59b5afe1fc4c18bc04ef424d36b0f3",
            "b855dbb8226e4a56a908c7a5ff34c592",
            "494b6148da4944a59cd6e4b2e957609c",
            "806d12d168624c78943290d89b5ad27b",
            "0b69e36d00cd40cfad8f8f856ec92609",
            "64b7abf8755844ffa51a6e292120f52c",
            "050a2d20b0a1455098edadd0db4ffa88",
            "c178626ae1f7417ba7dea78dd04e9a7b",
            "1dabd0c6286f47459e4be53b5176546d",
            "c013d1a5849b4a82a99f4a140e964c04",
            "07e5c8bfe7f6414cbd333f76da3bff39",
            "1dc13f72b3824c17a33e2749daf3ea13",
            "d596b9c38a8e4a82960b234bee0aec5f",
            "93a84feb0dd44ea1b328b12fd04a38bd",
            "c3ea969faee74d0db625f903dda225d2",
            "f3729ebc26c9459994608b7979f38975",
            "6aacf7d01af84a0e9ed3811d5da9625b",
            "e5bdf36689e74c7faabcee09896f878e",
            "c9474ab42b2842d68cefd6c2fa0878a6",
            "703a22acfc9c4aa196f330043429b37f",
            "6d8eccc9eec74d298628443d1e028031",
            "2a46e077ffe04e3bb691c647784cd04a",
            "d717557bc89342ad80ea95e1e0d28aa6",
            "a23aab212a054d7ab119c5bc27b1e5e5",
            "45cfb7fd382a4f3da511a99fdb525113",
            "95896cfe6da04d0e9c0c0fb7b26e498d",
            "a4dbee7998564ac4b3a1341e63ad271a",
            "59b321d661c145a2938198032fa82c95",
            "931d29190e1f4be2b87fb9fab9636cde",
            "ef4e722e992f4b6b9c8bab7797013d8d",
            "daffa0b044d1426a87b7decdf0c1d0c4",
            "2480468520f24674a1f69f4fdc58cc5d",
            "383e92b6bb844290a0bd8d28539619cb",
            "77c05c0eb3a64a4dbb1e0004c484265f",
            "239596964c5042f39cfefbd736ba2f2b",
            "3656bb5ba5f046209e24c3aba5c5b78e",
            "73c8fb4c4f8945b99cc69dcd1b5206aa",
            "828efd90b5e5472a9866ad5901e8418c",
            "c30de262a6534b8b8c876c6845c9da61",
            "ed27e952ce55441e9e4f7bdd8325961c",
            "f82fc807e46445c59890601507a04d6f",
            "1ccf4425da7a4492a934f13f64e92b60",
            "7e9dd657873d4126b4f30da434c50947",
            "94dd4bc514294c488a746611d8574fa1",
            "26c9216f46bc4142b5221d83d4adcd6e",
            "dca5c2b145dd4dfeb52a610f1e3d2aa3",
            "28567bd7fdec4e9d8ccdc2ef7bd452c4",
            "4be2b65e5b1f45b2ab949817c4d59da4",
            "b24c1660b7fb40cd84fd02b9b5818a1b",
            "106f23aed4b449cbb42fc3cc43770a81",
            "f9198eee77de4f3f9204abcc6b399f34",
            "53116399416e4368ab0f4e6640889f36",
            "7b8c98d31a0e40b7aa7e987b5ce32999",
            "21bbfd7897254ca8accf44625142495d",
            "7a6836f2edb04c91b557122ce414cb84",
            "f77730fcbf1145e4b1ca9af3b66911c4",
            "10967cd38c74434a9d359c0bc8a9657c",
            "d63b5eff3da14db5a33e1328152e0a25",
            "5597de34b562441a94f388243648261e",
            "8768aa4c0b804047b118bd2bd749ea11",
            "5adb2a894aab4559986518ca0752047f",
            "9dd768f7d36e40b5b8b493cc1bc29bd5",
            "26230c9e8322488faacb3e8fe584e58b",
            "0cd79f1dcb7e4b219f1e66e70aa5a5e1",
            "65004b8708c6421380124a91a57b6e67",
            "1b925ee44e5d4b0ba0e5159ca800bbe7",
            "42759857142d4f5e8ee568210b6f9fa8",
            "614f7aaa383c4f39ac85b99ce58cfe3c",
            "0975ca494478422885f5a3bfdf223eca",
            "a34dde08f1654ee98bd4c137e8b12620",
            "82be872348ac4e03836836a77f4ea34b",
            "fe774b9b266b413f97d16fd8e4710b57",
            "12002a94843046a59708e3279483f2bd",
            "348bf35a4cc1412c9f407549a59cac1c",
            "be73fd6367af4e6686d6b5771aa0e6fc",
            "eea6383f7d624e118ee05831aaa745fb",
            "a6dc161dadfb432da78bb3408cfd0c3f",
            "3d49382082e54dc8aaa6226f7da574c1",
            "8396a3e17b964b648450d5505032a3d5",
            "acaa6cca001a4cbbbf3905aa2dc164fc",
            "380dbd975d894540bab693585ec9973e",
            "d6269f21cae64b14bbad5b3650fcdd98",
            "288d2d1d83c94ebc805977ba6974e0be",
            "bb86c23a11764167b956bba7505a597c",
            "65635dc5a7664149896142942f9b350e",
            "fa2110f32cda4a479a1add5681f64f21",
            "84f1817e19a94856a2f4cc839166b5ae",
            "cc70ab7427904f7095f6bd1b42c23368",
            "e70d8f1afee047a59b4566dc10736dea",
            "df2764d9c8ee41259cd29264c439b740",
            "6c903adbbb044a7db98802fec4ecd159",
            "8ace49d781f74861af313a43da26504f",
            "b8ccd8aa2ff04640a33d09e36cecc609",
            "ad43258ed15f41eaa01dfcb506767182",
            "99bc5c5449d74fafa6368d3e312d518a",
            "e60b7abcd4fe4a32a0e9a3d6faf92684",
            "be4a377b10774db284bd7a1058d93f31",
            "795a9142fad94692b268239bba3350eb",
            "72523741f71d432c90b595f7bec08877",
            "7641f7ffbf724fbb8dbcebdd9a22c881",
            "1775f2aaa3834afeb62e8631aac778bc",
            "1220839f52574c03a851cc96e0433010",
            "db66ff4985a54bf4b13d1f164c816b96",
            "34a38403af8147b082207e4378093bec",
            "5bfa589bd9ab4957b68b9371f503de00",
            "d233faed501744c084b7fe9f5508414c",
            "85e1d378fc9d4bc89166dce7b343d290",
            "3a81443c7029496f9477b6d73f9900c7",
            "f8829fc3c3f14dee89f3273d5c11c13f",
            "ada11c6a64c647c386cf20dd195760f3",
            "2fdfe92376234651b90122dfd10b739f",
            "f6cada9fb47e4d378b040d06aa2b1778",
            "2af676f5564842edb8a2b83e576f14a3",
            "383f22051f084ec89d42fd072056ae9a",
            "765d410e57c7495ead57ddefaad24aba",
            "a048b44b44f14e6e9af1551b97db6ff1",
            "b7b58e2f81014fd687dd64c37c1538a4",
            "855e8d1e479546d4a1428c0b2e7ed741",
            "e160e14acf3c43c7be9924ae871d2b04",
            "acec396e162f4beaacf5b6caa2396d67",
            "0ab3e00cae454a6fbdb444009d99b3fc",
            "0b49d3b73fce4c75b4cd29437aebf4d2",
            "ddfb1c1906524245b640bcaaeea2abff",
            "85c5b6d01ccb4f4c824283e1b6f7ea2f",
            "6ce0f482f2434485b6068a68f23f4d34",
            "59cd9724e83f45ea988643006be6744a",
            "b3e1cecb1e9c49f1806d010146b5fddd",
            "ece6409bd791414d8489dc8fddc764e3",
            "421844092b2d4a7d9ff7d9a4b14748f6",
            "92a8cb88a7ce49399a461ea11ba9b038",
            "1f1d5d6146aa480bb8d0394b57e2c3a1",
            "0b60d573f5074dd68bb814393da0a6b9",
            "da4c49cb6ae8448cbc9239182adb33ae",
            "dbaf1e76b85d47bb945d39d0ec1cda53",
            "784b4e9c15bb442d90f421889e368b65",
            "8f8585fa5c2c4f06a01a4bed2ff15a0e",
            "1b232f912ebb436fb4fbc5f076d6f3c8",
            "2dea124c0c9a4d588f018fd68d31f757",
            "8fdd5677cd4c47dd85f3bd2ca76ce2cf",
            "ebf64f7d5fe04d67b0ec7deaeb957227",
            "c7e59ec04ba2481794a5830925b9568b",
            "79b5f8baf9984be5be69e9af4e6c6208",
            "710ab05e5e5240e78355a042ae541023",
            "029423ca85fb4585881879b3a1fda585",
            "69d6ca7c29ae47deb6b8cbaadee9b51f",
            "1e6fa026e3534b77bcdc94c8fb0924b7",
            "fc526b5cde024141b73db420ec11c568",
            "d336d312b59e41bf97fe0e2cb85c99e6",
            "dcc537e672554b97b5683d8db4bb5832",
            "838950e47bbe412d9d3d06ea991936ec",
            "8ccf233930c2426b8c64cb9052f77d2a",
            "9fcc3aedb5c94ccfa49336064b497907",
            "b361d9288ddf41cfb4eb57b7138c55f3",
            "49a9d736579847fa979247c7be05f9f6",
            "3141a19f41e144748de06c6865e17ce3",
            "dc4ab01d739043a496703224a2c4c924",
            "b609189a89a940ceaba77833eb012e60",
            "e45e8c3174ad4118af2ded9705e32d05",
            "0bffb74a2fe645e687cf0b718f020df5",
            "75cea02abb58437bace16569e4a7fa96",
            "8c82b3fefa6d44858b56f1d2bdfcc2ef",
            "d811ae240ee648b99977609fafaad5ed",
            "a5bf1f17d9a245de960e478024b509af",
            "36a791c38022481b95985d3534632046",
            "3c3dbb27c6704e3bb075c952cc9fe925",
            "ba6a6f3eac3f4d0a8ae87b2fadd2ff63",
            "664261ee2c44472fb04a7d07d1152347",
            "8d1071d9dc754a06834e0916fe3a7d77",
            "94019e701d5142a7908b3e49cd91eaf0",
            "e83db6b85f6a40deb48d4f67c4a8738b",
            "41c66d17143545a6a968253f1ee276b2",
            "01e5c3faa87b4e80969b8d05c4e5410a",
            "2edeb71e8b5948a783676674ad86b515",
            "3dbebea6ec584150b08b1ddb48c2ff84",
            "f74b7b03cfee475581cf08041b518dd1",
            "913a015f40bf40769b3d21b1524845e2",
            "2c499884905546aeb05c1cd1fd1dde7c",
            "81cf4a2038214568a6233a6c356b7d7b",
            "5ba9860a2ec14cda92c54c95bedba856",
            "c9db6534b1cf432688ee36801204336f",
            "4139cc8d5a7446bf8fe6b4f820b9acf4",
            "dfedc64599004541a987feb634cb26c2",
            "38d1e4111c514992a156881c0b0ba21f",
            "361f65180d6a4cdf9a3d5af418a88636",
            "af8ac081c8b241c6aaaceb1061dd6e9d",
            "fc8d4635d8a74affa601505d26cc106e",
            "24c4795b7854418ea27625521b61cf65",
            "d50864256b1e4877833e8c56b65a252d",
            "af914b17d6b84e7c92a651aae901df86",
            "81724003026e40568e4c4a2b62b429bb",
            "17cad70fb75a49619fda72256e77c469",
            "468ab55935a04b6991f147ee1a6957d2",
            "62302efc5d2d433790e3f87701bea16c",
            "7110b53bd323402591316d46388cbee5",
            "ceaaa3404f8e4e24b86950efc993d328",
            "3bcf692e9d5a411891f044d71a567fec",
            "fbeefd95bf3b4e2e90488cde2efdc1d7",
            "27f4e50a2ce943fab89f52a07a7e5c96",
            "7b8777ad54494e6e9b80c720259e1161",
            "72f51e55d231478699ea53c494d8505c",
            "cc31c4eaece2407989c7294e9db71ce9",
            "3bbcd3d63f5845c1b25287afc22c2294",
            "b16416c46bb04019ac65b2cf942b41fd",
            "484a7e8ca5724c77815ad06a07b8414e",
            "b9f946c0801d435ebb1dc5666bf6ead7",
            "79d4236b6a2148e9866d5103cdeb5ee6",
            "a6a10d4232fd445188a6549bb43d7682",
            "c9fd2a383bc8459cbbd962f2dd42888a",
            "e5413b488cf84c19a0372b3a37784e7f",
            "b4f719fe3be348b49e12a2f2c05e15f3",
            "dc338dbb4a39465cb0539388421c44de",
            "ba750777da8e4ac6b11ce1d95c4a7212",
            "70403a3a21224baf8e6a9b8f3a2514ea",
            "7681c41988bc4e098590c3fbe10ce945",
            "cdfb552f6bca429aa847b696dcf2c85a",
            "c69b3d1dd88a419198ca6d584b0f3477",
            "4988dd4c47544126afeff604149a16b9",
            "353a37390f4b43738851e7032ead1d59",
            "67f866a18e62490c81498ffce3d6aa13",
            "b65aa213343b43e68ece680870b382da",
            "01782114020749919da7da692d10b4c5",
            "08ffcb194a3a4430b9ba9ac336371ff8",
            "f256539f7c954210959ec5e4741fc68c",
            "5be64a23f9144fbbaa9b3b5cad8cbba4",
            "cabce36a678d4df18d6824dbf198a634",
            "ccd2c6fc91954d6d928c501ee8d70da0",
            "ae4522b0e1bc45a19343af2d5b500cd2",
            "a05697ac18fb46d684d8b775e428ccf5",
            "a50f9dee37e340ce81451fb8293701e4",
            "2ddc9e092a614edda83847e69c329656",
            "4d0c29e8527943d5a10470e81163b1a3",
            "5375b9ecbdf14a479efec79820295532",
            "1cbd973dba6842a4993daa1886e0abd9",
            "35f7dfbbbca7485fa33841a8b7309fba",
            "2db9efb0e3674088bf7b456437e2e8fa",
            "0d9c2a57d63b4750824a11508a8f2602",
            "5fa0e65cf3aa44a8822aa2a49b5d1251",
            "06fcb2a6974a4299baef1a1c1de9b03b",
            "643b5c99b50c46feb928edf9a0f8d672",
            "5c6d5aac4d0f46908f27066c04d95630",
            "af90a50db8c44b1b99e25e352ab40050",
            "ec1f6f79f5464e30a5cb2b755d9e7aeb",
            "5b709449e25e4f7d8aaf6f19fb4a14d4",
            "017851093adc48919343be7eeeffe53d",
            "72b4c2753905438bbb76f37a1db830d1",
            "974aba90904d45e8868a86e33c231ebc",
            "29c01593f3d54e9982ada1410a9d27ce",
            "04b8a746735a47b2891a5da51bf5f1a8",
            "9f2589d9b89846acbdf1b765df58582a",
            "ab5d8c735990417e859ab1591c3cce58",
            "b53c619099dd4a88808b7b6887597100",
            "23a7c3d5f0a543a297713b367a66282b",
            "88f52451fd7b41b6a76339a2e9fc8b34",
            "e25d1f298f754fcb847e59d940ea6b81",
            "a0e104084cca4ca49c47c2acbbae47d1",
            "06439c5b07d740f19528cfc24a0726c1",
            "6c52cbbd475d439f818454d11ea00aed",
            "b449dd595b1a463eba03cdd244fc873c",
            "dd84a01a8f5e4f88a847dc3df7b345e6",
            "1d3011107e734e4e9c585f9ed2a03f0e",
            "400d5f44216844059d24c6c0bab056d0",
            "82629cfd66054adcb9f3d533ed4d0056",
            "465661c3497f4ec08497f34c50edb3c3",
            "ffee8f053cb343e8a2aa9e85c91c6382",
            "db7a7959db3d4583bea8f5b132fa6f0d",
            "e44c0f576e824b3b81fbbc45564bf953",
            "78a04233db4c416fa2c50d839f92aee1",
            "61db6eebdb4741e288abc57a02f0dec6",
            "f2247f01c8e944e78495843092b4ad6e",
            "5d3bb2dd8ced451d8292d1645996c528",
            "80f80c26bfce4eeab9369582d29ced20",
            "2fa98f352eae4f78b66a1ab18c7b6694",
            "bbe23c6a779647668b51761f135b75d8",
            "a7244fbb6879414ca8d97680f7afed6b",
            "1f96cbb5a1894a3db753c1576de91cce",
            "ba50723175d648ee8fa985455c42a216",
            "4863c2d56de945729caf002848eddb73",
            "52109079f9544140a59067071ab3b289",
            "7618fd6ee4364dbc9e0bfe089ef6f71c",
            "5819278571724199884cb2455db12800",
            "fb2c5ffe88434bd88225017e4fc67968",
            "9a4964ac652a4090a84d95ea0a74795a",
            "2aa07f64694b458988b1546d17473640",
            "fb25e785060c4d38b30660e71b95f878",
            "205ce87c2d4b44dfaa6e169025a9c247",
            "1f054d485b7c41f7adb0d804dd4334cb",
            "d87ef893a1ec4a859437e30dd8832d94",
            "0dc99b5253f6457bb0aa9505eee74da9",
            "93a8ee8d56274813a8adfbe60c871bdb",
            "28ef67124fdf4add8b755fc295d57f44",
            "fa69d2ae934a41dd8e75f81a48855a93",
            "16a1eab7454c40f5906587839acf5883",
            "b7246517741743ae8e0043f3f5f4f7f9",
            "ddf211d2ead7477facba70b42996460a",
            "c0a14db63bce4be2ac4417757b119bc1",
            "7fb14f6f429f4cd99eb81f0963eba158",
            "d3b72ab6901c4097b9308222bc546025",
            "60692961dc2f44ba96cd0549ec0797e7",
            "7ef030942fbe4ceab5984656cefa90c4",
            "4cb154af69ed44f28b4730fe311c134d",
            "0823ad8be548430d91023296d011245c",
            "d6a11bb1f2c944008bdd41e609b318cf",
            "5623cbba860249e7b97f49a86d18d436",
            "16c144b73a484ea5b0d391cd3c6c36c3",
            "6d01317fccee445b92c5ede3fd675d84",
            "93573032685741a2940d70177b8198ad",
            "b8c45941d8af445b8106598f992fab21",
            "fc34716385aa4a8ca7cd5580d9ec682a",
            "0df0f95048ff4b279e0717b409828641",
            "e701410f866e4655817a65509f322432",
            "8051990a1103417781be9e09a26da8d2",
            "a57dd1a3fa4d4d60906daafbcab679d2",
            "221b9c332d284ac1a55f61bee78adad0",
            "cee688cfa5df4472a9c5fbbf3f5dd699",
            "7677e0d859a646409d569c88c59dc5cb",
            "f6cef8bd39ac4abe84a4e9b9ce1eb34c",
            "01c3027cd0b44812badd74bfb3eeacde",
            "f1da88c0dd714b50a6b0a290fb9340b8",
            "5f4acaa2db024eeabbd3ee4746ff836d",
            "9149c898492f4b67883e71e3de75e620",
            "52868761128b41c199857d5ddfa928e8",
            "b9b453c88275471e9b3ba90038b539fb",
            "3f1d111dc9a14910a252c0d93b5158a5",
            "24f979701e1c4555b068a7c6ff9e372f",
            "bdde7c302a274ed2951984ba1d9ad685",
            "922e18d5b8014722902bbab9f9fd2c7a",
            "208de93733044b989dbd5aaf70d1c3de",
            "19107e16f1984b89b1a5954d4588e8ad",
            "5091c5c05453466f8c59ba7dd2ede22e",
            "1c605ae3537e460090b3f6a0083d3487",
            "8fbd406ef5004f26af8b283bf54718d2",
            "1bd8fe12c2ae43aabc4b63d954e9dc48",
            "27876b71459a47ff9b9625e60d56d70f",
            "5e129a193b8f46d4b9e4683953493c9d",
            "812a11ec876e4bc883b61e505d8814c0",
            "871d4873b23e4d8588b765e6b0ab2891",
            "e1a78facd7ea4cb6a6c4d25d22c2cdd3",
            "a09e1ca5f98d4275bca845baa9d724df",
            "07981311b5ed4a479910a2b4887de21c",
            "6d74e854cd374b3ba89a686e1955c80d",
            "54514f6c0e3b4d769282e3dcb0c3375c",
            "4b138464524c47afa8e194a7ad45a43e",
            "cb03adbdf3114788bd7a0e80acfc5f2e",
            "df3682b906574b249f125a81555a0bda",
            "907c6f1ba2a9484bb24e18b84e6ffaf7",
            "bb07ac29020649cda24da236039258bd",
            "9829223d0975455d8879a1082afe91be",
            "2724fbb2cecb4a2b95cd7d172ef3d589",
            "5e5fb86b89084c6b810428b136cf9bc6",
            "02ac60cdfbdb48aabbf7e7d38aa5002e",
            "84a0c6a211b94f128ea7be31e6411b40",
            "bcb47cdbdb8b4faeb74e6c3b52dd166e",
            "f0bdf1f48c9540b99a1b0bcd3bf3cf9f",
            "aad283f0dea04ab0b4c5ebd4ed58a15d",
            "21f13954e6314158be43005a547852ad",
            "bd56c0b086c84b20bc2a58cb8930ed38",
            "9e06ddd313034947a486f07acdb65729",
            "7b0fb94c6ac9431287714c1a2ce5ea72",
            "f04292ea544a4bd681c42c55685b9447",
            "c1d55b8698a7442785527c32096937f9",
            "9bf892f1c609422a8f30039079a01118",
            "1a11b3761595429993c6f4b1f49cf1a5",
            "576099b685754b19b58117434c9da3a2",
            "84f5bb1a024e4c6b8a67ce79916d88d1",
            "db7bb6a976d549b089af07ebf014d838",
            "b5fc890b833e49329f61213b499afdc3",
            "476db52b86b34c31be349b7d2737dd91",
            "07ce4a117726477fada24362fffb0856",
            "04371ce578dc4dfda18e76cdd03db9c7",
            "2c49d329116b4011a905bac0d0fe823e",
            "d8a4e6af9fe844ea8cbfaa4c34fdf8f3",
            "5539cd8e23a74de08935f52656ecd3f2",
            "470d3ecdc9bf4f7e888a7a1b0a12bdb9",
            "b7ecb2b9c7c640eabd2c78e1ce51164a",
            "d1881276fa69477990d739335821ae13",
            "f4e344276df4414697ef6bc7b51b8cf7",
            "38d32e653a794d089a5298f28d946da1",
            "c41a9b1f9cd64bd3ab813baa3a6137af"
          ]
        },
        "id": "rXg1TJ5ABrmu",
        "outputId": "e01c4967-dce3-4bc5-d439-5f50a9c435cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type | Params | Mode \n",
            "---------------------------------------\n",
            "0 | model | Unet | 24.4 M | train\n",
            "---------------------------------------\n",
            "24.4 M    Trainable params\n",
            "0         Non-trainable params\n",
            "24.4 M    Total params\n",
            "97.745    Total estimated model params size (MB)\n",
            "188       Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66eb620550e34c98975a5fdf897e8aab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "157e4ceae7984f31949dc52c93fe810d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb30e004811544b4aea7961db2e7f90a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba6e6f946f0a433d9f16e405a71aba21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b69e36d00cd40cfad8f8f856ec92609"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3729ebc26c9459994608b7979f38975"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4dbee7998564ac4b3a1341e63ad271a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "828efd90b5e5472a9866ad5901e8418c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b24c1660b7fb40cd84fd02b9b5818a1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8768aa4c0b804047b118bd2bd749ea11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82be872348ac4e03836836a77f4ea34b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6269f21cae64b14bbad5b3650fcdd98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8ccd8aa2ff04640a33d09e36cecc609"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34a38403af8147b082207e4378093bec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "765d410e57c7495ead57ddefaad24aba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59cd9724e83f45ea988643006be6744a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b232f912ebb436fb4fbc5f076d6f3c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=15` reached.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d336d312b59e41bf97fe0e2cb85c99e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9330824613571167    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_iou         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7561880946159363    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.13053195178508759   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9330824613571167     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_iou          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7561880946159363     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.13053195178508759    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/135 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0bffb74a2fe645e687cf0b718f020df5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/14.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e83db6b85f6a40deb48d4f67c4a8738b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type          | Params | Mode \n",
            "------------------------------------------------\n",
            "0 | model | DeepLabV3Plus | 4.1 M  | train\n",
            "------------------------------------------------\n",
            "4.1 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.1 M     Total params\n",
            "16.544    Total estimated model params size (MB)\n",
            "231       Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4139cc8d5a7446bf8fe6b4f820b9acf4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "468ab55935a04b6991f147ee1a6957d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b16416c46bb04019ac65b2cf942b41fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7681c41988bc4e098590c3fbe10ce945"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cabce36a678d4df18d6824dbf198a634"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d9c2a57d63b4750824a11508a8f2602"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29c01593f3d54e9982ada1410a9d27ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b449dd595b1a463eba03cdd244fc873c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2247f01c8e944e78495843092b4ad6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5819278571724199884cb2455db12800"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa69d2ae934a41dd8e75f81a48855a93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6a11bb1f2c944008bdd41e609b318cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "221b9c332d284ac1a55f61bee78adad0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24f979701e1c4555b068a7c6ff9e372f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "812a11ec876e4bc883b61e505d8814c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb07ac29020649cda24da236039258bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e06ddd313034947a486f07acdb65729"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=15` reached.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07ce4a117726477fada24362fffb0856"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9262682795524597    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_iou         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7268131971359253    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.14089824259281158   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9262682795524597     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_iou          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7268131971359253     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.14089824259281158    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Интерпретация результатов Бейзлайна:**\n",
        "\n",
        "**Модель 1: U-Net (с энкодером ResNet34)**\n",
        "\n",
        "*   **Параметры модели:**\n",
        "    *   Общее количество параметров: 24.4 миллиона.\n",
        "    *   Размер модели: ~97.7 MB.\n",
        "    *   Это относительно крупная модель для U-Net с таким энкодером, что говорит о ее потенциальной способности к хорошему обучению на сложных данных.\n",
        "\n",
        "**Модель 2: DeepLabV3+ (с энкодером MIT-B0 - трансформерным)**\n",
        "\n",
        "*   **Параметры модели:**\n",
        "    *   Общее количество параметров: 4.1 миллиона.\n",
        "    *   Размер модели: ~16.5 MB.\n",
        "    *   Эта модель значительно легче (меньше параметров), чем U-Net с ResNet34. Это важно для развертывания на устройствах с ограниченными ресурсами."
      ],
      "metadata": {
        "id": "jf1z5LKtNxbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод:**\n",
        "1.  **U-Net с энкодером ResNet34 (сверточная архитектура):**\n",
        "    Модель имеет 24.4 миллиона параметров. Обучение проводилось в течение 15 эпох с использованием функции потерь DiceLoss и оптимизатора Adam. На последней эпохе обучения были достигнуты следующие показатели на валидационной выборке: `val_loss` = 0.125, `val_iou` = 0.772, `val_acc` = 0.931.\n",
        "    При оценке на тестовой выборке модель U-Net показала следующие результаты:\n",
        "    *   **Test IoU (mIoU): 0.756**\n",
        "    *   Test Pixel Accuracy: 0.933\n",
        "    *   Test Loss: 0.131\n",
        "\n",
        "2.  **DeepLabV3+ с энкодером MIT-B0 (архитектура с трансформерным энкодером):**\n",
        "    Данная модель является более легковесной, имея 4.1 миллиона параметров. Условия обучения были аналогичны модели U-Net (15 эпох, DiceLoss, Adam). На последней эпохе обучения показатели на валидационной выборке составили: `val_loss` = 0.138, `val_iou` = 0.741, `val_acc` = 0.922.\n",
        "    Результаты на тестовой выборке для модели DeepLabV3+ с энкодером MIT-B0:\n",
        "    *   **Test IoU (mIoU): 0.727**\n",
        "    *   Test Pixel Accuracy: 0.926\n",
        "    *   Test Loss: 0.141\n",
        "\n",
        "**Таким образом,**\n",
        " обе модели продемонстрировали способность к решению задачи бинарной семантической сегментации на выбранном датасете. Сверточная модель U-Net с энкодером ResNet34 показала несколько лучшие результаты по основной метрике IoU (0.756 против 0.727) на тестовой выборке по сравнению с моделью DeepLabV3+ с трансформерным энкодером MIT-B0. Однако, U-Net является значительно более тяжелой моделью по количеству параметров. Скорость обучения U-Net также была выше (около 41 секунды на эпоху против 55 секунд для DeepLabV3+). ба подхода продемонстрировали высокое качество сегментации, однако классическая сверточная архитектура оказалась более эффективной на данном датасете. Полученные значения IoU (72-75%) являются хорошей отправной точкой для дальнейших улучшений в рамках пункта 3 лабораторной работы."
      ],
      "metadata": {
        "id": "NzxHIiP1UcSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Улучшенный бейзлайн"
      ],
      "metadata": {
        "id": "b4h9-wjXhYBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ГИПОТЕЗЫ\n",
        "\n",
        "**H1: Более сильные аугментации (пространственные и цветовые)**\n",
        "\n",
        "Для повышения обобщающей способности моделей были применены расширенные аугментации изображений, включающие как пространственные (например, случайные обрезки, повороты, масштабирование), так и цветовые преобразования (изменение яркости, контраста, добавление шума и размытия). Это позволяет модели лучше справляться с разнообразием входных данных и уменьшает переобучение.\n",
        "\n",
        "\n",
        "**H2: Более мощные энкодеры (ResNet50 для U-Net, mit_b2 для DeepLabV3+)**\n",
        "\n",
        "Были использованы более глубокие и производительные энкодеры: ResNet50 для U-Net и mit_b2 (Vision Transformer из библиотеки timm) для DeepLabV3+. Это увеличивает способность модели извлекать сложные признаки из изображений, что особенно важно для задач сегментации с высоким разрешением и сложной структурой объектов.\n",
        "\n",
        "**H3: Комбинированная функция потерь (Dice + BCE)**\n",
        "\n",
        "В качестве функции потерь использовалась комбинация Dice Loss и Binary Cross-Entropy (BCE) с весами 0.7 и 0.3 соответственно. Dice Loss хорошо подходит для задач сегментации, так как напрямую оптимизирует перекрытие масок, а BCE способствует более стабильному обучению и учитывает пиксельную точность. Совместное использование этих функций позволяет достичь лучшего баланса между качеством сегментации и стабильностью обучения.\n",
        "\n",
        "**H4: Продвинутый планировщик скорости обучения (PolyLR) + AdamW**\n",
        "\n",
        "Для оптимизации обучения был выбран оптимизатор AdamW с weight decay, который помогает бороться с переобучением. В качестве планировщика скорости обучения использовался PolyLR (polynomial learning rate decay), который плавно уменьшает learning rate по мере обучения, что способствует более эффективной и стабильной сходимости модели."
      ],
      "metadata": {
        "id": "guTiYt3ghfsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n{'='*15} ЗАПУСК УЛУЧШЕННОГО БЕЙЗЛАЙНА {'='*15}\")\n",
        "# ================================================================\n",
        "# УЛУЧШЕННЫЕ АУГМЕНТАЦИИ, H1\n",
        "# ================================================================\n",
        "H, W = 736, 736\n",
        "\n",
        "train_tf_improved = A.Compose([\n",
        "    make_rrc(H, W, scale=(0.5, 1.0), p=1.0), # ИСПРАВЛЕНО: scale до 1.0\n",
        "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.7),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
        "    A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "test_tf_improved = A.Compose([\n",
        "    make_resize(H, W),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# ================================================================\n",
        "# LIGHTNING MODULE С УЛУЧШЕНИЯМИ\n",
        "# ================================================================\n",
        "MODELS_IMP = {\n",
        "    \"Unet_resnet50\": lambda: smp.Unet(\"resnet50\", encoder_weights=\"imagenet\", # H2\n",
        "                                      in_channels=3, classes=1, activation=None),\n",
        "    \"DeepLabV3Plus_mit_b2\": lambda: smp.DeepLabV3Plus( # H2\n",
        "        encoder_name=\"mit_b2\", encoder_weights=\"imagenet\",\n",
        "        in_channels=3, classes=1, activation=None),\n",
        "}\n",
        "\n",
        "# H3: Комбинированная функция потерь\n",
        "dice_loss_imp = smp.losses.DiceLoss(\"binary\", from_logits=True)\n",
        "bce_loss_imp = torch.nn.BCEWithLogitsLoss()\n",
        "def combo_loss_fn(logits, target):\n",
        "    return 0.7 * dice_loss_imp(logits, target) + 0.3 * bce_loss_imp(logits, target)\n",
        "\n",
        "# H4: PolyLR планировщик\n",
        "def poly_lr_scheduler_fn(step, max_steps, initial_lr_factor, power=0.9):\n",
        "    if step >= max_steps: # Prevent going to 0 or negative if max_steps is approximate\n",
        "        return 1e-6 / initial_lr_factor # Return a very small factor instead of 0\n",
        "    return initial_lr_factor * (1 - step / max_steps) ** power\n",
        "\n",
        "\n",
        "class LitSegImproved(pl.LightningModule):\n",
        "    def __init__(self, name, lr=3e-4, bs=8, total_epochs=15):\n",
        "        super().__init__()\n",
        "        self.model = MODELS_IMP[name]()\n",
        "        self.initial_lr = lr\n",
        "        self.bs = bs\n",
        "        self.total_epochs = total_epochs\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    # Data\n",
        "    def setup(self, stage=None):\n",
        "        # Используем DroneBinDS из бейзлайна\n",
        "        self.ds_train_imp = DroneBinDS(*split_files[\"train\"], train_tf_improved)\n",
        "        self.ds_val_imp   = DroneBinDS(*split_files[\"val\"],   test_tf_improved)\n",
        "        self.ds_test_imp  = DroneBinDS(*split_files[\"test\"],  test_tf_improved)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.ds_train_imp, batch_size=self.bs, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.ds_val_imp, batch_size=self.bs, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.ds_test_imp, batch_size=self.bs, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Loop\n",
        "    def shared_step(self, batch, tag):\n",
        "        x,y = batch\n",
        "        logits = self.model(x)\n",
        "        loss   = combo_loss_fn(logits, y) # H3\n",
        "        pred   = (torch.sigmoid(logits) >= .5).long()\n",
        "        tp, fp, fn, tn = smp.metrics.get_stats(pred, y.long(), mode=\"binary\")\n",
        "        iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
        "        acc = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\")\n",
        "        self.log_dict({f\"{tag}_loss\":loss, f\"{tag}_iou\":iou, f\"{tag}_acc\":acc}, prog_bar=True, on_step=(tag==\"train\"), on_epoch=True)\n",
        "        if tag == \"train\":\n",
        "            current_lr = self.lr_schedulers().get_last_lr()[0]\n",
        "            self.log(\"learning_rate\", current_lr, on_step=True, on_epoch=False, prog_bar=False)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self,b,_):   return self.shared_step(b,\"train\")\n",
        "    def validation_step(self,b,_): return self.shared_step(b,\"val\")\n",
        "    def test_step(self,b,_):       return self.shared_step(b,\"test\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.initial_lr, weight_decay=1e-5) # H4: AdamW\n",
        "\n",
        "        # Расчет общего количества шагов для PolyLR\n",
        "        # Оцениваем по длине датасета, так как trainer.num_training_batches еще не доступен здесь\n",
        "        num_train_samples = len(split_files[\"train\"][0])\n",
        "        num_batches_per_epoch = (num_train_samples // self.bs) + (1 if num_train_samples % self.bs != 0 else 0)\n",
        "\n",
        "        total_training_steps = num_batches_per_epoch * self.total_epochs\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "            optimizer,\n",
        "            lr_lambda=lambda step: poly_lr_scheduler_fn(step, total_training_steps, 1.0, power=0.9)\n",
        "        )\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# TRAIN & TEST УЛУЧШЕННОГО БЕЙЗЛАЙНА\n",
        "# ================================================================\n",
        "def run_improved(name, lr, total_epochs=15, batch_size=8):\n",
        "    logger = (pl.loggers.WandbLogger(project=\"seg_drone_improved\", name=name)\n",
        "              if USE_WANDB else True)\n",
        "\n",
        "    m = LitSegImproved(name, lr=lr, total_epochs=total_epochs, bs=batch_size)\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "        devices=1,\n",
        "        max_epochs=total_epochs,\n",
        "        logger=logger,\n",
        "        log_every_n_steps=10,\n",
        "        enable_checkpointing=False\n",
        "    )\n",
        "    trainer.fit(m)\n",
        "    trainer.test(m)\n",
        "\n",
        "# Запускаем эксперименты для улучшенного бейзлайна\n",
        "EPOCHS_IMPROVED = 15\n",
        "BATCH_SIZE_IMPROVED = 8\n",
        "\n",
        "print(f\"\\nЗапуск обучения улучшенных моделей на {EPOCHS_IMPROVED} эпох, batch_size={BATCH_SIZE_IMPROVED}...\")\n",
        "run_improved(\"Unet_resnet50\", lr=1e-4, total_epochs=EPOCHS_IMPROVED, batch_size=BATCH_SIZE_IMPROVED)\n",
        "run_improved(\"DeepLabV3Plus_mit_b2\", lr=2e-4, total_epochs=EPOCHS_IMPROVED, batch_size=BATCH_SIZE_IMPROVED)\n",
        "\n",
        "print(f\"\\n{'='*15} ОБУЧЕНИЕ УЛУЧШЕННОГО БЕЙЗЛАЙНА ЗАВЕРШЕНО {'='*15}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2b697fec0f794d328a9c2881d22a9426",
            "9109914ee04c4ff2b463b730c3cdc6d9",
            "2b6634e48b494286a8e791ab97b36d63",
            "cc9c5fdceb3f4ce7a09ab34222126c5d",
            "ffaa82ad120e4e5db4713fdd3c58f7ef",
            "2a7420eda68842a6b1e2ece4a33d0ec6",
            "0ffeb74a709e4cf88da9f1927c845392",
            "72a7f639989844f085bbbfa6fa60ed65",
            "62f5c1db26db462481cae6dcca7a4164",
            "fdbcf2bfd9c346988bd07d0c7ee53e8a",
            "c26e104a965f4d37b369a1fc9fd07e56",
            "a8a790b2cbe647f69a9d8cc7b6e28f13",
            "6411bd2afaf24c1b8bbc195591bd2554",
            "7fee8ad8812c4079995064034042aac4",
            "c5c5ffd52a7c422bacc85bed2998ab29",
            "4bb9324512b743e5b784951016378edf",
            "7377afb1ed63432fb4f78e22bd34a273",
            "aca0c8fbdc284a8e8c75707c2b646be1",
            "2a2d5a2293f74c159326deb85c841e46",
            "c7e4a6e41c934ff7a8e1479200f6866a",
            "a4855710a145433dbc5328172b27e41e",
            "be7fcd360ee84a0bae8c0bc653ece5ad",
            "1dcb3f6f9f7f414b8c8f1da7576868e7",
            "09c51a03b08c44b48b53b71ec9ea230d",
            "f3b211d7279f4785b7e067a68281cd67",
            "356a1d9ebe7f4fe494900f06afa252e0",
            "38e9494b7a774fdfad639497eab51f88",
            "7271d52a879145a4b664b48b083c3586",
            "833bdc816532417bbb90e5ec1481a3ba",
            "3cab12460cec430ba2824d10e5f0e387",
            "c18c635a0f6045639ee0e088dea4d43e",
            "340b07cd70064025b436186dca8b6ca6",
            "73359731f05b431d81941334f4d74ccd",
            "e23fbaadf38e40deac61cc96ad7240b7",
            "cccce70ff4ed422098b95a835f9c61c5",
            "b5eee1e592ee441295df39abc63e6aed",
            "a0114dc3e05b49da81b8d0ccc473d3a8",
            "322cee4a817b4d94805f723a9be46c8c",
            "d7e7250b5299462f9602fe04c21a1325",
            "ce3c6793b56e41838cad513ba3b9e786",
            "19d7b59bd82247eea3d5e71529a88845",
            "1d07d288fd344ae59e5d369081a5a4b8",
            "30255f40b8b345ee8f1fcc7c20b64449",
            "5d54d12a61f341e8ae27b6c52852ab24",
            "0082b8a71b8a4e728e3f7f0677f5ea08",
            "953a5f0f69a541ff9d41b23f67de2621",
            "003d1883f8354926842ac7efb1923576",
            "4768d724bb5e46f08edcd3c86d14570b",
            "61f5af0af83f4d179e3585cd31651bf6",
            "53520c8664d349e28c9fa942a8a51c97",
            "40d4486dc1544a4da18c614fe9df4198",
            "044a281cae3646099508246c78cdb1db",
            "f1295a7e7dda4290ac153fc2c906b43f",
            "1fd704cacf544d07b21ef53d50387d2a",
            "5cde7fa2f4aa4b6f99439528a396a014",
            "03fe175d66f940a7989de8acbe67ae51",
            "9b27e63142be44d8ae09c0790ca3881b",
            "c5dd5822a139407f904ea2194cba13d5",
            "56079e67c89c4fe4988e6526f6a1c6f4",
            "e480586800d94cbcbd0920d5ea0ed153",
            "4365aba195a04bb79eae46b0d23d7a08",
            "1be95af9f307458f820851e7869ba4ee",
            "ec73b0cab97c4ee1bcce76bfe7966310",
            "b1ae5030056844388f9c4dd68aeec239",
            "2165648f1cdc46bbb7f965fccacc26fd",
            "e258da7e2a9e4fa384f1557484a26aba",
            "51203368a4fe4f179e0187b39a53f7c8",
            "f9434d5429514ba481773f48938a4d23",
            "2fe648822d73440baed9b61a301a5db0",
            "ae5c900de7d6445fa526ff0c7a267762",
            "6a886c7076d94b46b88a359efeaf9fb1",
            "17537656e70348258642adb53a22a69a",
            "8266f0fa33d549989ecd0303847438a9",
            "d7a6537767404c359efc1a0db39ab779",
            "97b727c0a78c4210938039badc7de272",
            "b5466ed35e4b4e08a58c7d2c07e001dd",
            "b9153ef3fd8d4c38b9a7127fcab7ac1e",
            "6c8856e2d63a40fe996e6d3ef1cd760f",
            "ac29e87ca80346b0859375471628c301",
            "1e25d0e0e59a41f1b9fbfb66a7901e92",
            "325f46da19984f1bae04e068d306cef4",
            "af2addb5dc87487087e2611dd059dbd5",
            "26d7c049d5d1460894b3b38342e513a1",
            "f09efead994e4657aecab7e792e6f7db",
            "d89d855aa709413c95f599309a9ad8d1",
            "52045c45369c4f658dbeb948dfbc595d",
            "e0216a9915214c80b0b2ac7122ef5d2c",
            "4c4f0356025c470fa6f16ac52c6dccb0",
            "4e3a960513db4f3388c0b8573a7d0299",
            "ebe0aa4a86354df59fccde93b682cc04",
            "7b9cbb681e7048bc92d3e6fa9de05e29",
            "0a26b821ee49434cbee4e063aec43b7d",
            "758d270510d64899ac204f253b9c1d20",
            "cd7cac4fb057458d9e2191b07567c9d3",
            "97438d303acd49e1bf96b0b92d1f8527",
            "e138d903c64f42309131925596c80098",
            "0f3924886a2a48dc9a4fcbe866fdc820",
            "3f9c46e42a9c4d919b3f23e09f3fc30f",
            "d5bf5c197a6049b5b9dfa3a74131232e",
            "639e3a5a1efb4540a6e7160b6851b4fa",
            "bc09aeee45c341b9a07036d26ee469c7",
            "2d96fc80863141eb852394467f914058",
            "30572cd7d3be42d39131cd629c1ac200",
            "dd480a1dff924d53ad661c6b5b0e17b8",
            "98fe070b95754e6f95f67a8c00973952",
            "5c7dd72b20194a908e90a0f073c1dcb1",
            "949fb107079c41669ead91c2c98baaa7",
            "08025e54b1e849f0b0ec7093d08427d8",
            "6291515ee9f74e2c8daecf87a98afa2e",
            "a250ff0b4bd8414ea4ab78da1bc25756",
            "9fa75af17926419fa5993c72d9efd986",
            "89685685ca8b41b589c18b199b22ee63",
            "83eecc2f629d42c59adf73aba84a27df",
            "ec03e464204140c8b8fe3329e3e9e258",
            "c3e679b734e9454793ea5dd96d6aa284",
            "36138a84deec4629ad7d0cbc17b46558",
            "2655c635808e48f082ff9673238d5dd8",
            "e1bda89d4d0044348653d05a41ce1ada",
            "38d9f65f45874fc785edb1361d27e6a7",
            "eba28c97d0c14068b0e89313501dca62",
            "e222e9b4abd64f6a86e2ca8715a2d681",
            "047869ca8e2a43c38a3cf4872f889063",
            "c9d89a655a0a46b8b48d18dc133bbafb",
            "0957b1301b33434f8b5ac3edfa57a8f8",
            "0a980167bdff4e75ac557819566e4568",
            "261466d762bd4173bf3769062d39270f",
            "9bbfc0cfcb6746128b6925dd82f9767e",
            "16df6a46c47a44b3906f1673125570eb",
            "fe4800334ff14404942d1f30caa7893a",
            "ea5308023b9e4067adfb43b90d1a0e25",
            "e3902c2a5c9d4caab9baa1c07f81f470",
            "cf15955249574ea59ac38097d7086f9e",
            "bf82dded5ec14a9da175f079723ab35c",
            "656933504be6495ea4b986088ffd6811",
            "6dfff5ebb9084ecc8062e8d8998e0c51",
            "b98f99a6634b40d7a1b046762a07dd87",
            "cfad8fd5d5e8474ea0134c54b48c215d",
            "b7a99e53421448aeb68dc76dc8e94493",
            "62da4028afbe4498b3df0b64b7308138",
            "44c033bc014c4a63975743ddaccaf375",
            "28bf40c617f94ca6912215f04a574bfd",
            "8fcbf6c21ea642e6ace0609262c896ba",
            "d116e1a5a95e47b4b54dd96511e17026",
            "55969d1416e244d284578590bf6924d8",
            "bbe7fce6709c41159194ade17e783ac6",
            "6034e0220c49425ebb3d08dcc14bbfba",
            "cb15fe6204ec41c4afd129eb7841007c",
            "0905de7aca8b4d06822abc21a944d521",
            "4f16ccd2d25042f2a199fe9f164b5802",
            "3aeaf837bba64ef8a3c3f48e6a6ec373",
            "e2f3207b00d745be9f5d1893b5934945",
            "6979f67c925b4eb4839dc757af398663",
            "63e9c06d2796400987898728b586c889",
            "3cf5bc46d5e2439eb6a7889421430c94",
            "6da8973efa6743c4b7c72f0a3c81d42f",
            "dddd4b5a705444249c8bdb89f38ff5e7",
            "42fe3a0124194f09abbe005482706f87",
            "9cd200564159493dae85d94c3821cdac",
            "4b8d2068fdaa4922a21d9803121454a5",
            "5d77aba3c4bf4e84ad819e7a8a91ea08",
            "8d257716d0164e4b9c050e2b8c59e026",
            "7ef24db4284648c4b0113ffe19f8d96d",
            "c6ff33aa29b04c5988329d333524fc5f",
            "a38e8865cc7e4228ae9b88cb555d48ab",
            "4ac87ef7d6ff4202842f7449bd73b07c",
            "40337507735a40bc835083c6d795035e",
            "76b4df000e854661bc8a91eaaf506b00",
            "df6ec402f40e406ba42ea3e252ff154c",
            "d15e65d2a1b34c768222a3a154ab61c9",
            "ad9b24104c204a9cad08f1f392ebbcbc",
            "770bdf6fc5654ec4b05e3bc48edbaa34",
            "91b41db8d0fd48e7b0a0f95a0ce53285",
            "8941c1be1cf4452f841a6c7e1a0746a5",
            "43ffb6669b864742989ec90ee4caf22c",
            "584b4dddcbdd4cb0b5d7e455d5a7bade",
            "07c6c7ee3b55458f8121295cff119596",
            "d9b506b4da3e4b1fbe5cfd75e64906f5",
            "f00da53cd08645b09beb25960dc191cb",
            "e17c8fdf8bdd40ff93b04310a9c90748",
            "4834e14daef74b7e9b9c05cb88500c69",
            "017673d8f1034072b5257027b87668f6",
            "8a24377b0d0142b9aee72b8425aac366",
            "c83d50e336284724896ebfac19e2864b",
            "57174ba97c98488ba33372a9145f2261",
            "cfabf8fa3a49421f83d554ac91cea79f",
            "b932ebea35624c4da92f9c9c6f2599d9",
            "a074bc7ad4e641ec8634d23dd2577ee2",
            "395ebcb4f54e4587a9c51261f3e80410",
            "9446cbe045bf4f83a2369d47aab4dcdd",
            "08326d97df9b4a0b8c5de11bfb17a923",
            "8cbc4ddc52fe427a89f12587576e0826",
            "812bae2190054fbc92110af7196e85bc",
            "db9bd012b57d43058cdba04f6a4f3b98",
            "be22a083d1b84307a9245a6142b5807a",
            "e1986e562f714ac6b1b7b93462984051",
            "b594b44c234347779fcceef1730f0788",
            "ca65d6fc658a4390a5b5555cc8ea1578",
            "f57b99b36c3c4e8a8f83f2b3e0c9878f",
            "ae9e26401c8c40a29a9369382dcc50c0",
            "0b8d50086987456d85d59e3ecedf9c2a",
            "ca6359abc62f4cda9b91ba11f4a7e11d",
            "cacfa5d6a7724465ba1423216178c605",
            "27d47f6fcbb74f81a7ad0d0e6b886cfb",
            "1f1e3ef06d154209ae7f212e9bf4da24",
            "c01e4466831a46e68f1e61ef5c9cd4db",
            "7d698c540c4c4ec6a38b722e7430a081",
            "e91bb93696b04aba9ff9a3de8ad907a7",
            "08a438ae767a49ec876458bef29ff55f",
            "e0386ca42bb741deb03e6f2271f057d8",
            "8c0c313babdb4bd7ac90a440c8d97ac3",
            "b6438351d1e24e03bf149bcea1fa6bc3",
            "e27fdde841d249968f258b11efb36b6d",
            "10a119218bb64a86b82860fb73fa4c46",
            "1981af3aa3f845c299af491318b91fff",
            "59617931740e43d9a5a3ef64b78e1b80",
            "86f9d0566f8343739e33a9abf0c3255d",
            "12b6bd783e714811a1742c1ecc9785b4",
            "13655cf3c03d42c880d8be9f62ffe22c",
            "88176c592fde43dfa2f4dd48f11d209f",
            "339d9a178b464fa9af1e932b95a01dab",
            "5fade7f796034f28bb8abbbdd3d9252b",
            "2bb9b056be60473c94daa41b4a324485",
            "e5dc48688ec3405298fd417555c2e513",
            "d1e243615ca34f1e8906029b1247f2cf",
            "eba977e17bda4ceda372c7a0f302d1ee",
            "3ce1adaecfc84aa88708f07dc5284801",
            "1d548adebffe4439bfbb097d145f28d7",
            "869a520ae39144ca86a48204b2295dfc",
            "30c3be26c66a4243a263dede5f02e061",
            "f4962e4a46c64c3baf2899f1c2655480",
            "63e2a76406264e3c8e1441de7f5bf808",
            "add6ef421bc54f8eb4425119fd59df73",
            "8cac22c89aae4e9cb68a770b3e22b880",
            "3fde80cb039b48bf86360bdb538d9395",
            "6944d2775eb24311b5d5abd32082045c",
            "d378f031f82c49caae47f4bcb5ccf1b3",
            "6ef38aac1d2c44ed86403ed9825c0b30",
            "e41f8702573841bcbb2ea029ded08f89",
            "ea86adb26c46487fa08f695845566dd5",
            "76dadf9af1d94af88f4533aa5c6eadb8",
            "9cbb031a55c5480bbc06096c0eb6c04c",
            "55a2aee36b884b61a0ac101b4f13c09a"
          ]
        },
        "id": "rLecFYHcNbMe",
        "outputId": "80cf6822-4425-4b65-d589-27e9600dd6b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=============== ЗАПУСК УЛУЧШЕННОГО БЕЙЗЛАЙНА ===============\n",
            "\n",
            "Запуск обучения улучшенных моделей на 15 эпох, batch_size=8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-6552a8870382>:28: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type | Params | Mode \n",
            "---------------------------------------\n",
            "0 | model | Unet | 32.5 M | train\n",
            "---------------------------------------\n",
            "32.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "32.5 M    Total params\n",
            "130.084   Total estimated model params size (MB)\n",
            "223       Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b697fec0f794d328a9c2881d22a9426"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8a790b2cbe647f69a9d8cc7b6e28f13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1dcb3f6f9f7f414b8c8f1da7576868e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e23fbaadf38e40deac61cc96ad7240b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0082b8a71b8a4e728e3f7f0677f5ea08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03fe175d66f940a7989de8acbe67ae51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51203368a4fe4f179e0187b39a53f7c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c8856e2d63a40fe996e6d3ef1cd760f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e3a960513db4f3388c0b8573a7d0299"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "639e3a5a1efb4540a6e7160b6851b4fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fa75af17926419fa5993c72d9efd986"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "047869ca8e2a43c38a3cf4872f889063"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf82dded5ec14a9da175f079723ab35c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55969d1416e244d284578590bf6924d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6da8973efa6743c4b7c72f0a3c81d42f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40337507735a40bc835083c6d795035e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9b506b4da3e4b1fbe5cfd75e64906f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=15` reached.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "395ebcb4f54e4587a9c51261f3e80410"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9254651069641113    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_iou         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7329956293106079    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2021360844373703    \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9254651069641113     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_iou          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7329956293106079     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2021360844373703     </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/135 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae9e26401c8c40a29a9369382dcc50c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/98.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c0c313babdb4bd7ac90a440c8d97ac3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type          | Params | Mode \n",
            "------------------------------------------------\n",
            "0 | model | DeepLabV3Plus | 25.3 M | train\n",
            "------------------------------------------------\n",
            "25.3 M    Trainable params\n",
            "0         Non-trainable params\n",
            "25.3 M    Total params\n",
            "101.396   Total estimated model params size (MB)\n",
            "383       Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fade7f796034f28bb8abbbdd3d9252b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "add6ef421bc54f8eb4425119fd59df73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.07 GiB. GPU 0 has a total capacity of 14.74 GiB of which 698.12 MiB is free. Process 2724 has 14.06 GiB memory in use. Of the allocated memory 13.79 GiB is allocated by PyTorch, and 135.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6552a8870382>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nЗапуск обучения улучшенных моделей на {EPOCHS_IMPROVED} эпох, batch_size={BATCH_SIZE_IMPROVED}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0mrun_improved\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unet_resnet50\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS_IMPROVED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE_IMPROVED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0mrun_improved\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DeepLabV3Plus_mit_b2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS_IMPROVED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE_IMPROVED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n{'='*15} ОБУЧЕНИЕ УЛУЧШЕННОГО БЕЙЗЛАЙНА ЗАВЕРШЕНО {'='*15}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-6552a8870382>\u001b[0m in \u001b[0;36mrun_improved\u001b[0;34m(name, lr, total_epochs, batch_size)\u001b[0m\n\u001b[1;32m    138\u001b[0m     )\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# Вывод в стиле бейзлайна уже обеспечивается самим pl.Trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         )\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected state {self.state}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;31m# in automatic optimization, there can only be one optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# gradient update with accumulated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsume_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m_optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# model hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         call._call_lightning_module_hook(\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;34m\"optimizer_step\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/module.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \"\"\"\n\u001b[0;32m-> 1302\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_zero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_model_and_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/plugins/precision/precision.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;34m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mclosure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     def _clip_gradients(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/plugins/precision/precision.py\u001b[0m in \u001b[0;36m_wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mclosure_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclosure_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mClosureResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# unused hook - call anyway for backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-6552a8870382>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, b, _)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-6552a8870382>\u001b[0m in \u001b[0;36mshared_step\u001b[0;34m(self, batch, tag)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshared_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mloss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mcombo_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# H3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mpred\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segmentation_models_pytorch/encoders/mix_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_depth\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segmentation_models_pytorch/encoders/mix_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segmentation_models_pytorch/encoders/mix_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, height, width)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.07 GiB. GPU 0 has a total capacity of 14.74 GiB of which 698.12 MiB is free. Process 2724 has 14.06 GiB memory in use. Of the allocated memory 13.79 GiB is allocated by PyTorch, and 135.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ошибка CUDA out of memory для второй модели (DeepLabV3Plus_mit_b2) на бесплатном GPU Google Colab Tesla T4 с ~15 ГБ VRAM довольно ожидаема, так как полученные модели тяжеловесны, а трансформерные энкодеры, даже \"легкие\" как mit_b2, могут быть более требовательны к памяти, чем сверточные (ResNet50). Попробуем решить данную проблему. Перезапустим среду выполнения."
      ],
      "metadata": {
        "id": "t6qf6hAFUzVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 0. INSTALL DEPENDENCIES\n",
        "# ================================================================\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q pytorch-lightning segmentation_models_pytorch albumentations timm tqdm\n",
        "\n",
        "# ================================================================\n",
        "# 1. IMPORTS & ENVIRONMENT SETUP\n",
        "# ================================================================\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True' # Попытка уменьшить фрагментацию\n",
        "\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from pathlib import Path\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import segmentation_models_pytorch as smp\n",
        "import inspect # Для универсальных аугментаций\n",
        "import gc      # Для сборщика мусора\n",
        "USE_WANDB = False\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "id": "MFbnKU31aoVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 2. PATHS TO DATASET (на Google Drive)\n",
        "# ================================================================\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/Colab Notebooks/2/binary_dataset/binary_dataset\")\n",
        "# Локальная папка на диске Colab для копирования данных\n",
        "LOCAL_ROOT = Path(\"/content/dataset_local_binary_seg\")\n",
        "\n",
        "IMG_DIR_DRIVE  = DRIVE_ROOT / \"original_images\"\n",
        "MASK_DIR_DRIVE = DRIVE_ROOT / \"images_semantic\"\n",
        "\n",
        "IMG_DIR_LOCAL  = LOCAL_ROOT / \"original_images\"\n",
        "MASK_DIR_LOCAL = LOCAL_ROOT / \"images_semantic\"\n",
        "\n",
        "# ================================================================\n",
        "# 2.1 COPY DATASET TO LOCAL COLAB DISK (ВАЖНО ДЛЯ СКОРОСТИ И СТАБИЛЬНОСТИ)\n",
        "# ================================================================\n",
        "import shutil\n",
        "if LOCAL_ROOT.exists():\n",
        "    print(f\"Локальная копия {LOCAL_ROOT} уже существует, используем ее.\")\n",
        "else:\n",
        "    print(f\"Копируем данные из {DRIVE_ROOT} в {LOCAL_ROOT}...\")\n",
        "    try:\n",
        "        LOCAL_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "        items_to_copy = [item for item in DRIVE_ROOT.iterdir()]\n",
        "        for item in items_to_copy:\n",
        "            dest_path = LOCAL_ROOT / item.name\n",
        "            if item.is_dir():\n",
        "                shutil.copytree(item, dest_path)\n",
        "            else:\n",
        "                shutil.copy2(item, dest_path)\n",
        "\n",
        "        print(\"Копирование датасета на локальный диск завершено.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при копировании датасета: {e}\")\n",
        "        print(\"Убедитесь, что Google Drive примонтирован и путь корректен.\")\n",
        "        print(\"Проверьте также права доступа к папке Drive.\")\n",
        "        IMG_DIR_LOCAL  = IMG_DIR_DRIVE\n",
        "        MASK_DIR_LOCAL = MASK_DIR_DRIVE\n",
        "        print(\"Продолжаем работу с данными напрямую с Google Drive (может быть очень медленно и нестабильно).\")\n",
        "\n",
        "\n",
        "assert IMG_DIR_LOCAL.exists() and MASK_DIR_LOCAL.exists(), \\\n",
        "    f\"Проверьте путь к датасету: {IMG_DIR_LOCAL} или {MASK_DIR_LOCAL} не найдены после попытки копирования.\"\n",
        "\n",
        "# ================================================================\n",
        "# 3. COLLECT FILES + SPLIT (80/10/10)\n",
        "# ================================================================\n",
        "imgs  = sorted(list(IMG_DIR_LOCAL.glob(\"*.png\")) +\n",
        "               list(IMG_DIR_LOCAL.glob(\"*.jpg\"))) # Включаем оба популярных формата\n",
        "\n",
        "if not imgs:\n",
        "    raise RuntimeError(f\"В {IMG_DIR_LOCAL} не найдено изображений (*.png или *.jpg). Проверьте путь и содержимое.\")\n",
        "\n",
        "print(f\"Найдено {len(imgs)} изображений.\")\n",
        "\n",
        "masks = [MASK_DIR_LOCAL / f\"{p.stem}.png\" for p in imgs]\n",
        "existing_masks = [m_path for m_path in masks if m_path.exists()]\n",
        "if len(existing_masks) != len(imgs):\n",
        "     missing_masks = [m_path for m_path in masks if not m_path.exists()]\n",
        "     print(f\"Предупреждение: Найдено {len(imgs)} изображений, но только {len(existing_masks)} масок. Отсутствуют первые 5: {missing_masks[:5]}...\")\n",
        "     # Обновляем список изображений, чтобы соответствовать найденным маскам\n",
        "     imgs = [IMG_DIR_LOCAL / f\"{m.stem}{Path(imgs[0]).suffix}\" for m in existing_masks]\n",
        "masks = existing_masks\n",
        "\n",
        "if len(imgs) < 3: # Минимум для сплита train/val/test\n",
        "     raise RuntimeError(f\"Недостаточно данных ({len(imgs)} пар изображение/маска) для разбиения на train/val/test.\")\n",
        "\n",
        "perm  = np.random.permutation(len(imgs))\n",
        "tr_idx_end = int(.8*len(perm))\n",
        "val_idx_end = int(.9*len(perm))\n",
        "\n",
        "# Гарантия хотя бы одного элемента в каждом сплите (если данных мало)\n",
        "tr_idx_end = max(1, tr_idx_end)\n",
        "val_idx_end = max(tr_idx_end + 1, val_idx_end)\n",
        "if val_idx_end >= len(imgs):\n",
        "    val_idx_end = len(imgs) # Весь остаток идет в val/test\n",
        "\n",
        "split_files = dict(\n",
        "    train=(np.array(imgs)[perm[:tr_idx_end]],   np.array(masks)[perm[:tr_idx_end]]),\n",
        "    val  =(np.array(imgs)[perm[tr_idx_end:val_idx_end]], np.array(masks)[perm[tr_idx_end:val_idx_end]]),\n",
        "    test =(np.array(imgs)[perm[val_idx_end:]], np.array(masks)[perm[val_idx_end:]]),\n",
        ")\n",
        "\n",
        "if len(split_files[\"train\"][0]) == 0: raise RuntimeError(\"Обучающая выборка пуста после разбиения.\")\n",
        "if len(split_files[\"val\"][0]) == 0:   print(\"Предупреждение: Валидационная выборка пуста.\")\n",
        "if len(split_files[\"test\"][0]) == 0:  print(\"Предупреждение: Тестовая выборка пуста.\")\n",
        "\n",
        "print(f\"Разбиение: train={len(split_files['train'][0])}, val={len(split_files['val'][0])}, test={len(split_files['test'][0])} семплов.\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 4. DATASET CLASS (DroneBinDS) - Использует пути из split_files\n",
        "# ================================================================\n",
        "class DroneBinDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, imgs_paths, masks_paths, transform_fn):\n",
        "        self.imgs_paths = list(imgs_paths)\n",
        "        self.masks_paths = list(masks_paths)\n",
        "        self.transform_fn = transform_fn\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img  = cv2.cvtColor(cv2.imread(str(self.imgs_paths[i])), cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(str(self.masks_paths[i]), 0) # Читаем как grayscale\n",
        "        mask = (mask > 127).astype(\"float32\") # Преобразуем в бинарную 0/1\n",
        "\n",
        "        augmented = self.transform_fn(image=img, mask=mask)\n",
        "        image_tensor = augmented[\"image\"] # Ожидается tensor (C, H, W) от ToTensorV2\n",
        "        mask_tensor  = augmented[\"mask\"]  # Ожидается tensor (H, W) от ToTensorV2\n",
        "\n",
        "        # Добавляем канал к маске (ожидается (1, H, W) для DiceLoss/BCEWithLogitsLoss)\n",
        "        if mask_tensor.ndim == 2:\n",
        "             mask_tensor = mask_tensor.unsqueeze(0)\n",
        "\n",
        "        return image_tensor, mask_tensor.float() # Маска тоже float для лосса\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 5. AUGMENTATIONS (Улучшенный вариант, совместимый с Albumentations)\n",
        "# ================================================================\n",
        "# Возвращаемся к исходному разрешению 736x736\n",
        "H_IMG, W_IMG = 736, 736\n",
        "\n",
        "# Функции make_resize_fn и make_rrc_fn\n",
        "def make_resize_fn(h, w):\n",
        "    sig_params = inspect.signature(A.Resize).parameters\n",
        "    if \"height\" in sig_params and \"width\" in sig_params: return A.Resize(height=h, width=w)\n",
        "    elif \"size\" in sig_params: return A.Resize(size=(h, w))\n",
        "    else: raise TypeError(\"Не удалось определить API для A.Resize\")\n",
        "\n",
        "def make_rrc_fn(h, w, **kwargs):\n",
        "    sig_params = inspect.signature(A.RandomResizedCrop).parameters\n",
        "    if \"height\" in sig_params and \"width\" in sig_params: return A.RandomResizedCrop(height=h, width=w, **kwargs)\n",
        "    elif \"size\" in sig_params: return A.RandomResizedCrop(size=(h, w), **kwargs)\n",
        "    else: raise TypeError(\"Не удалось определить API для A.RandomResizedCrop\")\n",
        "\n",
        "\n",
        "train_transforms_improved = A.Compose([\n",
        "    make_rrc_fn(H_IMG, W_IMG, scale=(0.5, 1.0), p=1.0),\n",
        "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.7),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
        "    A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
        "    A.GaussNoise(p=0.3),\n",
        "    A.Normalize(), # Выполняет нормализацию\n",
        "    ToTensorV2(), # Конвертирует в Tensor\n",
        "])\n",
        "\n",
        "test_transforms_improved = A.Compose([\n",
        "    make_resize_fn(H_IMG, W_IMG),\n",
        "    A.Normalize(), # Выполняет нормализацию\n",
        "    ToTensorV2() # Конвертирует в Tensor\n",
        "])\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 6. LIGHTNING MODULE (LitSegImproved)\n",
        "# ================================================================\n",
        "# Используем только модель DeepLabV3Plus с mit_b2\n",
        "MODELS_TO_TRAIN = {\n",
        "    \"DeepLabV3Plus_mit_b2\": lambda: smp.DeepLabV3Plus(\n",
        "        encoder_name=\"mit_b2\", encoder_weights=\"imagenet\",\n",
        "        in_channels=3, classes=1, activation=None),\n",
        "}\n",
        "\n",
        "# Комбинированная функция потерь\n",
        "dice_loss_fn = smp.losses.DiceLoss(\"binary\", from_logits=True)\n",
        "bce_loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "def combined_loss(logits, target):\n",
        "    return 0.7 * dice_loss_fn(logits, target) + 0.3 * bce_loss_fn(logits, target)\n",
        "\n",
        "# PolyLR планировщик\n",
        "def poly_lr_scheduler(step, max_steps, initial_lr_factor, power=0.9):\n",
        "    if step >= max_steps:\n",
        "        return 1e-7 # Очень маленькое значение вместо 0, чтобы не обнулять совсем\n",
        "    return initial_lr_factor * (1 - step / max_steps) ** power\n",
        "\n",
        "class LitSegImproved(pl.LightningModule):\n",
        "    def __init__(self, model_key_name, initial_lr=3e-4, batch_size=8, num_total_epochs=15):\n",
        "        super().__init__()\n",
        "        self.model = MODELS_TO_TRAIN[model_key_name]()\n",
        "        self.initial_lr = initial_lr\n",
        "        self.batch_size = batch_size\n",
        "        self.num_total_epochs = num_total_epochs\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataset = DroneBinDS(*split_files[\"train\"], train_transforms_improved)\n",
        "        self.val_dataset   = DroneBinDS(*split_files[\"val\"],   test_transforms_improved)\n",
        "        self.test_dataset  = DroneBinDS(*split_files[\"test\"],  test_transforms_improved)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # Увеличен num_workers до 4 для потенциального ускорения загрузки с локального диска\n",
        "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    def shared_step(self, batch, stage_name):\n",
        "        images, true_masks = batch\n",
        "        logits = self.model(images)\n",
        "        loss   = combined_loss(logits, true_masks)\n",
        "\n",
        "        pred_masks = (torch.sigmoid(logits) >= 0.5).long()\n",
        "        true_masks_long = true_masks.long()\n",
        "\n",
        "        tp, fp, fn, tn = smp.metrics.get_stats(pred_masks, true_masks_long, mode=\"binary\")\n",
        "        # reduction=\"micro\" - усреднение по батчу (и по изображениям внутри батча)\n",
        "        iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
        "        accuracy_score = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\")\n",
        "\n",
        "        # log_dict автоматически добавляет префикс stage_name\n",
        "        self.log_dict({f\"{stage_name}_loss\":loss, f\"{stage_name}_iou\":iou_score, f\"{stage_name}_acc\":accuracy_score},\n",
        "                      on_step=(stage_name==\"train\"), on_epoch=True, prog_bar=True)\n",
        "\n",
        "        if stage_name == \"train\":\n",
        "            # Get LR from the first optimizer's first param group\n",
        "            if self.trainer.optimizers and len(self.trainer.optimizers) > 0:\n",
        "                current_lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
        "                self.log(\"learning_rate\", current_lr, on_step=True, on_epoch=False, prog_bar=False)\n",
        "            # else: LR scheduler might not be set up yet for the very first step\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch_data, batch_idx):   return self.shared_step(batch_data,\"train\")\n",
        "    def validation_step(self, batch_data, batch_idx): return self.shared_step(batch_data,\"val\")\n",
        "    def test_step(self, batch_data, batch_idx):       return self.shared_step(batch_data,\"test\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.initial_lr, weight_decay=1e-5)\n",
        "\n",
        "        # Расчет общего количества шагов для PolyLR\n",
        "        # Опираемся на train_dataset\n",
        "        # Добавляем проверку, что train_dataset доступен\n",
        "        if not hasattr(self, 'train_dataset') or self.train_dataset is None or len(self.train_dataset) == 0:\n",
        "             print(\"Warning: train_dataset not initialized or is empty. Cannot configure LR scheduler properly.\")\n",
        "             # Вернуть только оптимизатор или пропустить планировщик\n",
        "             return {\"optimizer\": optimizer}\n",
        "\n",
        "        num_train_samples = len(self.train_dataset)\n",
        "        # Убедимся, что batch_size > 0 во избежание деления на ноль\n",
        "        effective_batch_size = self.batch_size * self.trainer.accumulate_grad_batches if hasattr(self.trainer, 'accumulate_grad_batches') else self.batch_size\n",
        "        if effective_batch_size <= 0: effective_batch_size = 1 # Запасной вариант\n",
        "\n",
        "        num_batches_per_epoch = (num_train_samples + effective_batch_size - 1) // effective_batch_size # Целочисленное деление с округлением вверх\n",
        "\n",
        "        total_training_steps = num_batches_per_epoch * self.num_total_epochs\n",
        "\n",
        "        if total_training_steps <= 0:\n",
        "             print(\"Warning: total_training_steps is zero or negative. Cannot configure LR scheduler properly.\")\n",
        "             return {\"optimizer\": optimizer}\n",
        "\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "            optimizer,\n",
        "            lr_lambda=lambda step: poly_lr_scheduler(step, total_training_steps, 1.0) # 1.0 т.к. initial_lr уже в оптимайзере\n",
        "        )\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 7. FUNCTION TO RUN A SINGLE EXPERIMENT\n",
        "# ================================================================\n",
        "def run_single_experiment(model_key, learning_rate, epochs_count, current_physical_batch_size, use_amp=False, grad_accum_steps=1):\n",
        "    logger_to_use = (pl.loggers.WandbLogger(project=\"seg_drone_improved\", name=f\"{model_key}_bs{current_physical_batch_size}{'_amp' if use_amp else ''}{'_acc' + str(grad_accum_steps) if grad_accum_steps > 1 else ''}\")\n",
        "                     if USE_WANDB else True) # True for default Lightning logger\n",
        "\n",
        "    model_instance = LitSegImproved(model_key_name=model_key, initial_lr=learning_rate,\n",
        "                                   num_total_epochs=epochs_count, batch_size=current_physical_batch_size)\n",
        "\n",
        "    trainer_config_params = {\n",
        "        \"accelerator\": \"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "        \"devices\": 1,\n",
        "        \"max_epochs\": epochs_count,\n",
        "        \"logger\": logger_to_use,\n",
        "        \"log_every_n_steps\": 10,\n",
        "        \"enable_checkpointing\": False,\n",
        "        \"accumulate_grad_batches\": grad_accum_steps # Передаем сюда накопление\n",
        "    }\n",
        "\n",
        "    if use_amp:\n",
        "        trainer_config_params[\"precision\"] = \"16-mixed\"\n",
        "\n",
        "    # Убираем accumulate_grads из аргументов run_single_experiment\n",
        "    # Он теперь напрямую передается в trainer_config_params\n",
        "\n",
        "    trainer_instance = pl.Trainer(**trainer_config_params)\n",
        "\n",
        "    print(f\"\\n--- Обучение улучшенной модели: {model_key} ---\")\n",
        "    # Используем current_physical_batch_size для отображения\n",
        "    effective_batch_size = current_physical_batch_size * grad_accum_steps\n",
        "    print(f\"Параметры: LR={learning_rate}, Эпох={epochs_count}, Физ.батч={current_physical_batch_size}, Эфф.батч={effective_batch_size}, AMP={use_amp}, Разрешение={H_IMG}x{W_IMG}\")\n",
        "\n",
        "    # Запускаем обучение и тестирование\n",
        "    trainer_instance.fit(model_instance)\n",
        "    print(f\"\\n--- Тестирование улучшенной модели: {model_key} ---\")\n",
        "    trainer_instance.test(model_instance)\n",
        "\n",
        "    del model_instance, trainer_instance # Освобождаем память\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# ================================================================\n",
        "# 8. RUN EXPERIMENT FOR DeepLabV3Plus_mit_b2 (самые агрессивные настройки)\n",
        "# ================================================================\n",
        "EPOCHS_COUNT_DLV3 = 15 # Количество эпох\n",
        "MODEL_KEY_DLV3 = \"DeepLabV3Plus_mit_b2\"\n",
        "INITIAL_LR_DLV3 = 2e-4 # Начальный LR\n",
        "\n",
        "print(f\"\\n{'='*15} ЗАПУСК УЛУЧШЕННОГО БЕЙЗЛАЙНА ДЛЯ {MODEL_KEY_DLV3} {'='*15}\")\n",
        "\n",
        "# Настройки для DeepLabV3+ mit_b2 с учетом ошибок и памяти\n",
        "# Используем физический батч > 1 (например, 2), AMP, и Accumulation\n",
        "# Эффективный батч = current_physical_batch_size * grad_accum_steps\n",
        "run_single_experiment(\n",
        "    model_key=MODEL_KEY_DLV3,\n",
        "    learning_rate=INITIAL_LR_DLV3,\n",
        "    epochs_count=EPOCHS_COUNT_DLV3,\n",
        "    current_physical_batch_size=2, # ФИЗИЧЕСКИЙ БАТЧ > 1, например 2. Это обходит ошибку Batch=1, 1x1 spatial\n",
        "    use_amp=True,                  # Mixed Precision (сильно экономит память)\n",
        "    grad_accum_steps=4             # Накопление градиентов (эффективный батч = 2 * 4 = 8)\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*15} ОБУЧЕНИЕ МОДЕЛИ {MODEL_KEY_DLV3} ЗАВЕРШЕНО {'='*15}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789,
          "referenced_widgets": [
            "90571ee875044d7598924844be93ae52",
            "430aa7e78e104f20876c3731acb59482",
            "dc1bd7dac0ae4025b83d891952ef3aba",
            "a192bf3127054d5bac55e75dd4e85db8",
            "b8c89e94041e4622b047fad46d8a841e",
            "e9b22d8072c441299e34634b2c83a378",
            "f19659aef6644d4e8f40ac801dda2e71",
            "2d343727488948e6947bdcaa27cc6394",
            "2f2a74a50f074d9ca5facc5313fe9bcb",
            "1c4605cf7f644343ab8ea697598234e0",
            "69347df61ac14353b5bf0ef3e85315ec",
            "046ba0b002f94e12a74a3d18896bf152",
            "892cfea199bd45f5a80d74ad1e2c0bbd",
            "f1f28d500e814a1f8f693df8c5454709",
            "1479dab43f98493fb3337743f1ddcac3",
            "796edb01bbdb4e789e224b7d6781137c",
            "92b67f0b7c4a451abd3a036e000a863e",
            "bd7e9591eb1f418dbfd74b081c671f54",
            "2412971fcff84674bc2b4b394d8ef4db",
            "c8a7bce36c26486e90d571551e2c6694",
            "e134632f241945dcaa058ac3371679b2",
            "7f7418a8b25e42cc93191b84629e2c09",
            "4546d4e2e3b446129102c12fc2af266b",
            "99922a0e69a14243ac3ce9506b1ed47e",
            "ccb6ed616e24429192c393dc1a8ffd20",
            "68f2225e663049ae9c4455c7adc5a322",
            "a3b1403f6c4a495faae910ae9534f98a",
            "e90c0d06fb55461f9ab9d47a6d36b087",
            "7c77bb7f5e8a496fa806bf7c9f2270e7",
            "07b026e6971a48afa9edb538a60e9035",
            "d95f38860b6c46fea181afd808506180",
            "06a627957d1e4054b478ad02b5456db4",
            "a5e930c93f2a49b58b32eedd2adc4498",
            "0e256ba7c28f4551a5fe1fe2f1583b4b",
            "df9c7b62d56c44bca01aabb9239115a3",
            "b840b10b041a4e33bb4829c99e42c629",
            "d316e9e5eacb46be90c6ca5ef42796aa",
            "4378a9e1378e4493b63381bf2d847884",
            "d91d686032fa4ab4bed5e18a630cb40e",
            "80879360c0b44266a28e5c0f34552808",
            "c13f305025d7423fb95f4bb59a0b3e54",
            "fc2d129895fc45b39b2d4d2e927ae408",
            "13d9e44f2894484aa3ff5b6fdffc3704",
            "43075f1071dc48aba3cd674e03cfd878",
            "7ef9cbf7f81f4272be51fedfcf84ac3f",
            "d281be722cb14de8886ea0140f393461",
            "89fdbb5b32a0493fb1b2bb5037e7088f",
            "6bf8622838a041a1b62293fd3002838a",
            "3560a07f5114454ea2c1b56d1c59b563",
            "d54122689bba4688a6b1d501a0001d96",
            "2243c3c87e1b4a558cb52e494078f1e6",
            "d955c726f6e84180afc72559c6a50c65",
            "bc78ce28fc1c4d96b3deb41398fd0d34",
            "56d09b81a89c41fab022fce552ab315b",
            "2180ba6041b049d78f90bbc83d8c1b1c",
            "bdbe5728e19b4fbeb9bfa6b63e55b9f3",
            "4e0a6560668c4a308f15bdff4ffa4073",
            "60f16e39a5bf49c6ac6c66788c94b96a",
            "074b0aee0f78452b81711280e1d91194",
            "08f652411f644cfaabe179a98b8571aa",
            "7b67e10d2192401291b7e03ed7b8cc54",
            "4e5547df7ebe4c81aebcdbbb88617232",
            "07096aae3b694d45a79bc2a5f45fd2de",
            "bca4ab22e98d45c385ce030de76cebd7",
            "d600e93fe3ad4a0f83166903a72e0e14",
            "ce1ffc5be68a4d3dba6a4a44248f9e57",
            "0839c1d1313a433e8c2f258c9de91459",
            "8cf79ca880be4144aff985569105cc13",
            "651430f739da4b438162820262ee0b4a",
            "f211b4d0ba764091a8c24731d607150b",
            "25b2731770f24b6390e2c6c242d86795",
            "c463bb13a313409691a13679ace48a8d",
            "db04c7146914491f92d21625a6e2e19a",
            "6ec6bb1173f043a7b503967d553c9257",
            "17a994ea64de4fe29c57fdff49f1d42e",
            "6bd03cd6309243718b8f8108fbd64d68",
            "8326b9829fc1493aa5b98399bb4b8270",
            "8a8147a3d7364c99a6412b95b80286ad",
            "8894a57894164d409206b02ad33a42ef",
            "a522bacd31464858a278bba04343dec1",
            "31af7ed9863b47999f67ee7f609e80e3",
            "6675c9ab69414bb9868d48c7e33e78ba",
            "b1828c5cc4b044e78adc4dc4f96af65e",
            "3e0239a3f189403a9a0913528fcd2af9",
            "215a1834a30345c4974439edb279c577",
            "7c82d414fa5844acbefd4eabeb2cf8ee",
            "b00fe34ab8f843ac8b55ad6747b26ab5",
            "98f97450d60141a4b09c075ba3e633fb",
            "725c8420a5534087ad88f9f095c88c75",
            "1699914e6f214cdf8513313674e856fe",
            "dc7f665c8ce149a39bee3b6ecea57135",
            "a0b1d833781b413aa7c3b41f216c1237",
            "fc09bd6289444581aa23445e0ca5e249",
            "ac18d71036f5434f8ab386b71b39526c",
            "1634106487b648e39f985c706ef0aed4",
            "5cc295ab9f1b4ad380e0d5ddcc1aa69c",
            "3dcb80597aec400b8db8077ce3a136a6",
            "2a5fb70705c5488796ed3714f591dfb8",
            "f8a15f0572434d73852ba098ac233654",
            "ff6c7d238f484ae8b46f1a6ba907f235",
            "7cb7a7433f5145378b652de7407b33f8",
            "67245601e5144a39a761533a613c21fc",
            "924f5e5f6dc349c786010cc2159080f2",
            "c4ed721a0ad24ca4ad4e0e4d18d038b2",
            "ec34b9c9629949dfa89a63eaae0ea551",
            "96f85f7e1f4b44118a65172eed43d4f3",
            "f1a2423bb6b44178abd736d47fda0a52",
            "60f4fcebfc5e4f548b1abce2abcdd6f2",
            "c17fdb3058c2435aac675aa286c715f8",
            "7531c5867aa4486cbc5b52cf3dfaca46",
            "c2089f5135254411a854658966713f25",
            "70eb5758eb9d4c74a6c00343cb507333",
            "df87e997019347858b4e391ff0932826",
            "c5e78d66419a4d09b4302923da4da4fd",
            "dc1ad9ae9e784148a96b2f53bee216a3",
            "cbb0c6de741a4ef0aa46cbcd5ff7f6fe",
            "99c034d535574a2e83be400fe8cb2777",
            "2ae4847f4e7c41b682f0b8e616ee1882",
            "8835dcbba6614648a37334be60897218",
            "bed39884485a4a7c8053ee47b3ab93d4",
            "7eff04ae09d3484baa6f528c2d7cbf6f",
            "4cd8f6a406e842a39b4d0dc6bf97c06d",
            "56b6336b04a84b09a229527cffff8cc5",
            "05d0174375f9477ea7d5a6bde47ab8cf",
            "801b9d1bd9c3403ab11f42609e248807",
            "7ed900e0ae7743808aef3e3909c9a779",
            "c6a6c1cd1944413ba15e6423fc800c51",
            "845e89f51c4a43e2ba0f929991d32698",
            "b408f16cd497445e9e2d7aead0d38d33",
            "db0ea25163804dd2ac52be3948ee4d00",
            "32eae10807d942ab85742a0deceebf20",
            "4c6577e020c34d7d9033fcb2b1ed6c42",
            "43302e53ac7544838c567a833b5e6a1b",
            "8fc9a621f3414923989aacff667a3c4d",
            "619c4c1f68ff43f493deb1db16eab586",
            "ac0ab7654fe24feb84ac7c101542089b",
            "4ac0261551ed4305b374262cda39478a",
            "27126b7337bb41568d43e767bac197e4",
            "db14413a6e3947bb8770c4f3834ae87d",
            "f06d03ad6dc94fa19306cf4aa976be02",
            "9618be5f2f5746298e76a98fd5f9e429",
            "9b76786d7b3d491f8972135c63f7e573",
            "1af2835a285c4467aeae0b84e0f227f9",
            "315f4ae1edd141aba2ac43bd1d89e5ec",
            "8c6521f15dc2466da146ff884a060b24",
            "748db914defc499ca5a122a937a03cf4",
            "ddced75ccbf645089b9fa71c8795e801",
            "6efbad9e2fd744c086ffa527b90a8890",
            "64a4e91ddfb74a179e70dc17bd95909b",
            "00ad066515b046b2916ae745db92e4fb",
            "b917ea8809214245bfde3a08479e6dff",
            "47547d59412f4b809adaa7bc5c58cd90",
            "bbb76a4935ea45839f2902b6d14b8046",
            "4f2375cad0494bb99100983a4a91466a",
            "786a72aa156a4069bee9925a986dd15e",
            "82353b1640444c08bea2482bcee60746",
            "ee5c07b96592443794ef487e2aee0143",
            "f6b96666d2164e06beba6ec691cb4d26",
            "30b6090a7dec44a789eb1d4cad19279e",
            "457b7d96c478466e9ce6f6a3745ff6c6",
            "5f5bd636c2944580a73c8d09609b7c2c",
            "24fd8dc294f04493a42ea71850c61fff",
            "0e610cdd38274b2aaf490e613e1df197",
            "04c7b8ae9b2e437694f19feeabe5eb25",
            "a4a489c6095f4381bd72e69c5ad91f59",
            "d3d45b0b3b4b4230ac38757f64e3d920",
            "7e7d990069cc4055965d21e819f348c2",
            "dc6688c941474f3db65b61a30eb12c35",
            "e256c455acb648fc847d1189a0e6f433",
            "95876e90154049c0b6a110a73a17f90b",
            "22b7bc63d85b43848f1438778716b7a6",
            "2a0a1420ef804d819a97b1717e933297",
            "5a05cba3fbfa41d8b146be297e4f9549",
            "9fc433caa943432e9759f57ed8ee534f",
            "2e65fb02108242c6b0d09c38f766fa40",
            "dbfbc1f83c124e319d5a6be9886166cc",
            "693568f18f8346ffbd28af5cd2e86697",
            "08412c9583984f60911b00faa8b616a9",
            "130ee799a4b042ed93b8238f8103b00e",
            "b52bea2424ef4ff5893298727b9cb297",
            "80dd8120bc1e4a84bd5d389ea33fa507",
            "e6e0c4bd00ae47f6a87ff392844a49dc",
            "b98501c819b94e779ac3d6f1edc78d3d",
            "4265e5e67ec0487e8ca8201ddc9f0452",
            "c142e2df60ca40498f7650daa2fdd9cf",
            "72cd2f990be04499ba8450f89acc380d",
            "be46d7e91d034eaf982a4f6d03026152",
            "d1334c80bb8c4966820dc39f1dd96d9c",
            "c77ae9eee2064d86993c5ea533472847",
            "0eeb4858f1e74237a109e88bf3b262f9",
            "ee0ac1939a58474fa059d80b6a559293",
            "370f2e340b714986b97f2267ae8eabf4",
            "d431408abeb74557bcbd1e54df17b2cd",
            "bfba806da9644ccdac05aa5db6dcec91",
            "128d9827320c46f2b784b7367b05672f",
            "a7c845b70cfe47a18231a57190f6c8f8",
            "e106c0db73ad47f8aca426929331e713",
            "aa070b7d84f94bc8bb2f36ecf973069d"
          ]
        },
        "id": "4sH0GXJ0Y3NU",
        "outputId": "16e12766-e036-4388-9ff8-fa3bddfe4573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Локальная копия /content/dataset_local_binary_seg уже существует, используем ее.\n",
            "Найдено 400 изображений.\n",
            "Разбиение: train=320, val=40, test=40 семплов.\n",
            "\n",
            "=============== ЗАПУСК УЛУЧШЕННОГО БЕЙЗЛАЙНА ДЛЯ DeepLabV3Plus_mit_b2 ===============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type          | Params | Mode \n",
            "------------------------------------------------\n",
            "0 | model | DeepLabV3Plus | 25.3 M | train\n",
            "------------------------------------------------\n",
            "25.3 M    Trainable params\n",
            "0         Non-trainable params\n",
            "25.3 M    Total params\n",
            "101.396   Total estimated model params size (MB)\n",
            "383       Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Обучение улучшенной модели: DeepLabV3Plus_mit_b2 ---\n",
            "Параметры: LR=0.0002, Эпох=15, Физ.батч=2, Эфф.батч=8, AMP=True, Разрешение=736x736\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90571ee875044d7598924844be93ae52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "046ba0b002f94e12a74a3d18896bf152"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4546d4e2e3b446129102c12fc2af266b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e256ba7c28f4551a5fe1fe2f1583b4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ef9cbf7f81f4272be51fedfcf84ac3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bdbe5728e19b4fbeb9bfa6b63e55b9f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0839c1d1313a433e8c2f258c9de91459"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a8147a3d7364c99a6412b95b80286ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "725c8420a5534087ad88f9f095c88c75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff6c7d238f484ae8b46f1a6ba907f235"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2089f5135254411a854658966713f25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cd8f6a406e842a39b4d0dc6bf97c06d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43302e53ac7544838c567a833b5e6a1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "315f4ae1edd141aba2ac43bd1d89e5ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "786a72aa156a4069bee9925a986dd15e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3d45b0b3b4b4230ac38757f64e3d920"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "693568f18f8346ffbd28af5cd2e86697"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=15` reached.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Тестирование улучшенной модели: DeepLabV3Plus_mit_b2 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1334c80bb8c4966820dc39f1dd96d9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9283722639083862    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_iou         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.756996750831604    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1764976680278778    \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9283722639083862     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_iou          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.756996750831604     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1764976680278778     </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=============== ОБУЧЕНИЕ МОДЕЛИ DeepLabV3Plus_mit_b2 ЗАВЕРШЕНО ===============\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Во втором коде для обучения тяжёлой модели DeepLabV3+ mit_b2 были внесены оптимизации: данные копируются на локальный диск для ускорения доступа, физический размер батча снижен до 2 для экономии памяти, добавлено накопление градиентов (accumulate_grad_batches=4) для увеличения эффективного размера батча, включена смешанная точность (AMP) для снижения потребления GPU-памяти, увеличено число потоков загрузки данных (num_workers=4), а также реализовано явное освобождение памяти после эксперимента — всё это позволило избежать ошибок Out of Memory и ValueError при обучении на ограниченных ресурсах Google Colab."
      ],
      "metadata": {
        "id": "EiNFT93ImdUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Результаты**\n",
        "\n",
        "**Модель 1: U-Net с энкодером ResNet50, улучшенный бейзлайн**  \n",
        "- **Параметры:** 32.5 млн (увеличение на 33% от базовой U-Net ResNet34), Размер модели: ~130 МБ\n",
        "- **Тестовые метрики:**  \n",
        "  - mIoU = 0.732 (ниже, чем базовый U-Net ResNet34 — 0.756).  \n",
        "  - Pixel Accuracy = 0.925 (близко к базовой модели).  \n",
        "  - Loss = 0.202 (хуже, чем базовый 0.131).  \n",
        "\n",
        "**Модель 2: DeepLabV3+ с энкодером mit_b2 (трансформер), улучшенный бейзлайн**  \n",
        "- **Параметры:** 25.3 млн (увеличение на 565% от базового DeepLabV3+ mit_b0), Размер модели: ~101 МБ\n",
        "- **Тестовые метрики:**  \n",
        "  - mIoU = 0.757 (на 3% выше, чем базовый DeepLabV3+ mit_b0 — 0.727).  \n",
        "  - Pixel Accuracy = 0.928 (на 0.002 выше базовой модели).  \n",
        "  - Loss = 0.176 (значительно лучше базового 0.141).  "
      ],
      "metadata": {
        "id": "EJnZ7_4dncPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Выводы и сравнение"
      ],
      "metadata": {
        "id": "tZm8v7G1mt8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Сравнение с базовым бейзлайном:**\n",
        "\n",
        "| Метрика               | U-Net ResNet34 (база) | U-Net ResNet50 (улучш.) | DeepLabV3+ mit_b0 (база) | DeepLabV3+ mit_b2 (улучш.) |\n",
        "|-----------------------|-----------------------|-------------------------|--------------------------|---------------------------|\n",
        "| **mIoU (Test)**       | **0.756**             | 0.732 (↓3.2%)           | 0.727                   | **0.757 (↑4.1%)**         |\n",
        "| **Pixel Accuracy**    | 0.933                | 0.925 (↓0.8%)           | 0.926                   | 0.928 (↑0.2%)             |\n",
        "| **Loss (Test)**       | 0.131                | 0.202 (↑54%)            | 0.141                   | **0.176 (↓25%)**          |\n",
        "| **Параметры**         | 24.4M                 | 32.5M (↑33%)            | 4.1M                    | 25.3M (↑565%)             |\n",
        "| **Скорость обучения** | ~41 сек/эпоха         | Не указано              | ~55 сек/эпоха            | ~4 сек/шаг (эфф. батч=8)  |\n",
        "\n",
        "**Ключевые выводы:**\n",
        "1. **DeepLabV3+ mit_b2** превзошел базовую версию (mit_b0) по всем метрикам, демонстрируя преимущества более мощного трансформерного энкодера и улучшенных гиперпараметров (комбинированная функция потерь, PolyLR, накопление градиентов).  \n",
        "2. **U-Net ResNet50** проиграл базовой U-Net ResNet34 по mIoU и Loss, что может быть связано с переобучением или избыточной сложностью модели.\n",
        "3. **Скорость обучения** DeepLabV3+ mit_b2 (эффективный батч=8) улучшилась из-за оптимизации памяти (AMP, накопление градиентов), несмотря на большие параметры.  \n",
        "4. **Трансформерные модели** (DeepLabV3+ mit_b2) показали более высокую стабильность: при увеличении параметров на 565% метрики выросли, тогда как сверточная U-Net ResNet50 ухудшилась.  \n",
        "5. **Необходима оптимизация U-Net ResNet50:** Нужна перенастройка гиперпараметров, менее агрессивные аугментации, в целом более тонкая настройка для более сложного энкодера ResNet50."
      ],
      "metadata": {
        "id": "6YUqLjIQpgLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Самостоятельная реализация"
      ],
      "metadata": {
        "id": "RQvXtgAkhbw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Baseline"
      ],
      "metadata": {
        "id": "mbjIoCyPtmjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 1. ИМПОРТЫ\n",
        "# ================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# ================================================================\n",
        "# 2. ПОДГОТОВКА ДАННЫХ\n",
        "# ================================================================\n",
        "# --- Параметры ---\n",
        "DATA_ROOT = Path(\"/kaggle/input/semantic-segmentation-drone-dataset/binary_dataset/binary_dataset\")\n",
        "IMG_DIR = DATA_ROOT / \"original_images\"\n",
        "MASK_DIR = DATA_ROOT / \"images_semantic\"\n",
        "H, W = 512, 512\n",
        "\n",
        "# --- Сбор файлов ---\n",
        "imgs = sorted(set(list(IMG_DIR.glob(\"*.png\")) + list(IMG_DIR.glob(\"*.jpg\"))))\n",
        "masks = [MASK_DIR / f\"{p.stem}.png\" for p in imgs]\n",
        "pairs = [(i, m) for i, m in zip(imgs, masks) if m.exists()]\n",
        "\n",
        "print(f\"IMG_DIR: {IMG_DIR}, MASK_DIR: {MASK_DIR}\")\n",
        "print(f\"Найдено изображений: {len(imgs)}\")\n",
        "print(f\"Найдено пар изображение-маска: {len(pairs)}\")\n",
        "if len(pairs) == 0:\n",
        "    raise RuntimeError(\"Не найдено ни одной пары изображение-маска! Проверьте имена файлов и структуру папок.\")\n",
        "\n",
        "imgs, masks = zip(*pairs)\n",
        "\n",
        "# --- Разделение на train/val/test ---\n",
        "idxs = np.random.permutation(len(imgs))\n",
        "n = len(imgs)\n",
        "n_train = int(0.8 * n)\n",
        "n_val = int(0.1 * n)\n",
        "split_files = {\n",
        "    \"train\": (np.array(imgs)[idxs[:n_train]], np.array(masks)[idxs[:n_train]]),\n",
        "    \"val\":   (np.array(imgs)[idxs[n_train:n_train+n_val]], np.array(masks)[idxs[n_train:n_train+n_val]]),\n",
        "    \"test\":  (np.array(imgs)[idxs[n_train+n_val:]], np.array(masks)[idxs[n_train+n_val:]])\n",
        "}\n",
        "\n",
        "# ================================================================\n",
        "# 3. АУГМЕНТАЦИИ\n",
        "# ================================================================\n",
        "def make_resize(h, w):\n",
        "    return A.Resize(height=h, width=w)\n",
        "\n",
        "def make_rrc(h, w, scale=(0.5, 1.0), p=1.0):\n",
        "    return A.RandomResizedCrop(size=(h, w), scale=scale, p=p)\n",
        "\n",
        "train_tf_improved = A.Compose([\n",
        "    make_rrc(H, W, scale=(0.5, 1.0), p=1.0),\n",
        "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.7),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
        "    A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "test_tf_improved = A.Compose([\n",
        "    make_resize(H, W),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# ================================================================\n",
        "# 4. КЛАСС ДАТАСЕТА\n",
        "# ================================================================\n",
        "class DroneBinDS(Dataset):\n",
        "    def __init__(self, imgs_paths, masks_paths, transform_fn):\n",
        "        self.imgs_paths = list(imgs_paths)\n",
        "        self.masks_paths = list(masks_paths)\n",
        "        self.transform_fn = transform_fn\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img = cv2.cvtColor(cv2.imread(str(self.imgs_paths[i])), cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(str(self.masks_paths[i]), 0)\n",
        "        mask = (mask > 127).astype(\"float32\")\n",
        "        augmented = self.transform_fn(image=img, mask=mask)\n",
        "        image_tensor = augmented[\"image\"]\n",
        "        mask_tensor = augmented[\"mask\"]\n",
        "        if mask_tensor.ndim == 2:\n",
        "            mask_tensor = mask_tensor.unsqueeze(0)\n",
        "        return image_tensor, mask_tensor.float()\n",
        "\n",
        "# ================================================================\n",
        "# 5. МЕТРИКИ\n",
        "# ================================================================\n",
        "def iou_score(pred, target, threshold=0.5, eps=1e-6):\n",
        "    pred = (pred > threshold).float()\n",
        "    target = (target > threshold).float()\n",
        "    intersection = (pred * target).sum(dim=(1,2,3))\n",
        "    union = (pred + target - pred * target).sum(dim=(1,2,3))\n",
        "    iou = (intersection + eps) / (union + eps)\n",
        "    return iou.mean().item()\n",
        "\n",
        "def dice_score(pred, target, threshold=0.5, eps=1e-6):\n",
        "    pred = (pred > threshold).float()\n",
        "    target = (target > threshold).float()\n",
        "    intersection = (pred * target).sum(dim=(1,2,3))\n",
        "    dice = (2 * intersection + eps) / (pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3)) + eps)\n",
        "    return dice.mean().item()\n",
        "\n",
        "def pixel_accuracy(pred, target, threshold=0.5):\n",
        "    pred = (pred > threshold).float()\n",
        "    target = (target > threshold).float()\n",
        "    correct = (pred == target).float().sum()\n",
        "    total = torch.numel(pred)\n",
        "    return (correct / total).item()\n",
        "\n",
        "# ================================================================\n",
        "# 6. U-NET: САМОСТОЯТЕЛЬНАЯ РЕАЛИЗАЦИЯ\n",
        "# ================================================================\n",
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=1, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "        for feat in features:\n",
        "            self.downs.append(UNetBlock(in_ch, feat))\n",
        "            in_ch = feat\n",
        "        self.bottleneck = UNetBlock(features[-1], features[-1]*2)\n",
        "        for feat in reversed(features):\n",
        "            self.ups.append(nn.ConvTranspose2d(feat*2, feat, 2, stride=2))\n",
        "            self.ups.append(UNetBlock(feat*2, feat))\n",
        "        self.final = nn.Conv2d(features[0], out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skips = []\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skips.append(x)\n",
        "            x = F.max_pool2d(x, 2)\n",
        "        x = self.bottleneck(x)\n",
        "        skips = skips[::-1]\n",
        "        for i in range(0, len(self.ups), 2):\n",
        "            x = self.ups[i](x)\n",
        "            skip = skips[i//2]\n",
        "            if x.shape != skip.shape:\n",
        "                x = F.interpolate(x, size=skip.shape[2:])\n",
        "            x = torch.cat([skip, x], dim=1)\n",
        "            x = self.ups[i+1](x)\n",
        "        return self.final(x)\n",
        "\n",
        "# ================================================================\n",
        "# 7. ОБУЧАЮЩИЙ И ВАЛИДАЦИОННЫЙ ЦИКЛ\n",
        "# ================================================================\n",
        "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for imgs, masks in tqdm(loader, desc=\"Train\", leave=False):\n",
        "        imgs, masks = imgs.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # === Mixed Precision (AMP) для экономии памяти ===\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model(imgs)\n",
        "            loss = loss_fn(logits, masks)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "        # === Очистка кэша после каждого батча (дополнительно) ===\n",
        "        torch.cuda.empty_cache()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    iou, dice, acc, loss_sum = 0, 0, 0, 0\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in tqdm(loader, desc=\"Val\", leave=False):\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "            logits = model(imgs)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            loss = loss_fn(logits, masks)\n",
        "            iou += iou_score(probs, masks)\n",
        "            dice += dice_score(probs, masks)\n",
        "            acc += pixel_accuracy(probs, masks)\n",
        "            loss_sum += loss.item()\n",
        "    n = len(loader)\n",
        "    return {\n",
        "        \"iou\": iou/n,\n",
        "        \"dice\": dice/n,\n",
        "        \"acc\": acc/n,\n",
        "        \"loss\": loss_sum/n\n",
        "    }\n",
        "\n",
        "# ================================================================\n",
        "# 8. ОБУЧЕНИЕ И ТЕСТИРОВАНИЕ\n",
        "# ================================================================\n",
        "BATCH_SIZE = 2\n",
        "NUM_WORKERS = 2\n",
        "EPOCHS = 20\n",
        "\n",
        "train_ds = DroneBinDS(*split_files[\"train\"], train_tf_improved)\n",
        "val_ds   = DroneBinDS(*split_files[\"val\"],   test_tf_improved)\n",
        "test_ds  = DroneBinDS(*split_files[\"test\"],  test_tf_improved)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet(in_ch=3, out_ch=1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# === Mixed Precision (AMP) scaler ===\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "best_val_iou = 0\n",
        "best_model_state = None\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    torch.cuda.empty_cache()  # Очистка памяти перед эпохой\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
        "    val_metrics = evaluate(model, val_loader, device)\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_metrics['loss']:.4f} | Val IoU: {val_metrics['iou']:.4f} | Val Dice: {val_metrics['dice']:.4f} | Val Acc: {val_metrics['acc']:.4f}\")\n",
        "    if val_metrics['iou'] > best_val_iou:\n",
        "        best_val_iou = val_metrics['iou']\n",
        "        best_model_state = model.state_dict()\n",
        "\n",
        "print(\"\\n=== Оценка на тестовой выборке ===\")\n",
        "model.load_state_dict(best_model_state)\n",
        "test_metrics = evaluate(model, test_loader, device)\n",
        "print(f\"Test Loss: {test_metrics['loss']:.4f} | Test IoU: {test_metrics['iou']:.4f} | Test Dice: {test_metrics['dice']:.4f} | Test Acc: {test_metrics['acc']:.4f}\")"
      ],
      "metadata": {
        "id": "QcJQp2J_tlpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f9f93f-f6f2-4bcb-e832-34982de438bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMG_DIR: /kaggle/input/semantic-segmentation-drone-dataset/binary_dataset/binary_dataset/original_images, MASK_DIR: /kaggle/input/semantic-segmentation-drone-dataset/binary_dataset/binary_dataset/images_semantic\n",
            "Найдено изображений: 400\n",
            "Найдено пар изображение-маска: 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n",
            "<ipython-input-4-6999ccb1e9a5>:64: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
            "<ipython-input-4-6999ccb1e9a5>:236: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/160 [00:00<?, ?it/s]<ipython-input-4-6999ccb1e9a5>:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5124 | Val Loss: 0.4630 | Val IoU: 0.0002 | Val Dice: 0.0005 | Val Acc: 0.7784\n",
            "\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4722 | Val Loss: 0.4633 | Val IoU: 0.0393 | Val Dice: 0.0721 | Val Acc: 0.7855\n",
            "\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4651 | Val Loss: 0.4565 | Val IoU: 0.1140 | Val Dice: 0.1944 | Val Acc: 0.7620\n",
            "\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4417 | Val Loss: 0.4316 | Val IoU: 0.0735 | Val Dice: 0.1309 | Val Acc: 0.7769\n",
            "\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4483 | Val Loss: 0.4617 | Val IoU: 0.2914 | Val Dice: 0.4268 | Val Acc: 0.7871\n",
            "\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4322 | Val Loss: 0.4069 | Val IoU: 0.1486 | Val Dice: 0.2449 | Val Acc: 0.7979\n",
            "\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4264 | Val Loss: 0.4464 | Val IoU: 0.2518 | Val Dice: 0.3835 | Val Acc: 0.7877\n",
            "\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4308 | Val Loss: 0.4359 | Val IoU: 0.0997 | Val Dice: 0.1666 | Val Acc: 0.7840\n",
            "\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4141 | Val Loss: 0.4390 | Val IoU: 0.3891 | Val Dice: 0.5358 | Val Acc: 0.7966\n",
            "\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4212 | Val Loss: 0.4457 | Val IoU: 0.3906 | Val Dice: 0.5392 | Val Acc: 0.7740\n",
            "\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4105 | Val Loss: 0.3953 | Val IoU: 0.3724 | Val Dice: 0.5198 | Val Acc: 0.8075\n",
            "\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4015 | Val Loss: 0.5496 | Val IoU: 0.3956 | Val Dice: 0.5454 | Val Acc: 0.7482\n",
            "\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4085 | Val Loss: 0.4103 | Val IoU: 0.3404 | Val Dice: 0.4869 | Val Acc: 0.8208\n",
            "\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4018 | Val Loss: 0.4051 | Val IoU: 0.1994 | Val Dice: 0.3161 | Val Acc: 0.8028\n",
            "\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3914 | Val Loss: 0.4591 | Val IoU: 0.2330 | Val Dice: 0.3591 | Val Acc: 0.7948\n",
            "\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3956 | Val Loss: 0.3978 | Val IoU: 0.3706 | Val Dice: 0.5192 | Val Acc: 0.8099\n",
            "\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4034 | Val Loss: 0.4577 | Val IoU: 0.2140 | Val Dice: 0.3353 | Val Acc: 0.7986\n",
            "\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3987 | Val Loss: 0.3826 | Val IoU: 0.3314 | Val Dice: 0.4738 | Val Acc: 0.8259\n",
            "\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3987 | Val Loss: 0.4443 | Val IoU: 0.2163 | Val Dice: 0.3403 | Val Acc: 0.7883\n",
            "\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3905 | Val Loss: 0.4081 | Val IoU: 0.3831 | Val Dice: 0.5320 | Val Acc: 0.8124\n",
            "\n",
            "=== Оценка на тестовой выборке ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.3690 | Test IoU: 0.3656 | Test Dice: 0.5143 | Test Acc: 0.8303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В процессе снова была получена ошибка OutOfMemoryError: CUDA out of memory"
      ],
      "metadata": {
        "id": "-0zcTyfxdeiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Что использовано для экономии памяти:\n",
        "* Уменьшен размер изображений (H, W = 512, 512)\n",
        "* Уменьшен BATCH_SIZE = 2\n",
        "* Уменьшен NUM_WORKERS = 2 и pin_memory=False\n",
        "* Добавлен AMP (Mixed Precision) через torch.cuda.amp.autocast() и GradScaler\n",
        "* После каждого батча и эпохи вызывается torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "5n02gzBdh9gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Качество сегментации собственной реализации U-Net существенно ниже, чем у библиотечных моделей SMP.\n",
        "Разница по IoU составляет более чем в два раза (0.366 против 0.756 у U-Net ResNet34).\n",
        "Точность по пикселям и Dice также заметно ниже.\n",
        "Loss на тесте выше, что говорит о менее уверенных предсказаниях.\n",
        "Это ожидаемо, так как библиотечные реализации используют более мощные энкодеры (ResNet34/50, MiT), предобученные веса и более продвинутые техники оптимизации.\n",
        "\n",
        "Причины разницы:\n",
        "В собственной реализации используется \"чистый\" U-Net без предобученного энкодера и без глубоких архитектурных улучшений.\n",
        "В библиотечных моделях применяются современные энкодеры, предобученные на ImageNet, что даёт значительный прирост качества.\n",
        "Также в SMP-реализациях используются более сложные функции потерь и scheduler'ы."
      ],
      "metadata": {
        "id": "tI0PumYZqghV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Улучшение Baseline"
      ],
      "metadata": {
        "id": "uDhis1Htttjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Главные улучшения:\n",
        "\n",
        "* Более сильные аугментации (пространственные + цветовые)\n",
        "* Комбинированная функция потерь (Dice + BCE)\n",
        "* AdamW + PolyLR (реализация вручную)\n",
        "* AMP (Mixed Precision)\n",
        "* Всё оптимизировано под экономию памяти (batch=2, H,W=512, torch.cuda.empty_cache())"
      ],
      "metadata": {
        "id": "q0BN-l75m_aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 1. ИМПОРТЫ\n",
        "# ================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# ================================================================\n",
        "# 2. ПОДГОТОВКА ДАННЫХ\n",
        "# ================================================================\n",
        "DATA_ROOT = Path(\"/kaggle/input/semantic-segmentation-drone-dataset/binary_dataset/binary_dataset\")\n",
        "IMG_DIR = DATA_ROOT / \"original_images\"\n",
        "MASK_DIR = DATA_ROOT / \"images_semantic\"\n",
        "H, W = 512, 512\n",
        "\n",
        "imgs = sorted(set(list(IMG_DIR.glob(\"*.png\")) + list(IMG_DIR.glob(\"*.jpg\"))))\n",
        "masks = [MASK_DIR / f\"{p.stem}.png\" for p in imgs]\n",
        "pairs = [(i, m) for i, m in zip(imgs, masks) if m.exists()]\n",
        "\n",
        "print(f\"IMG_DIR: {IMG_DIR}, MASK_DIR: {MASK_DIR}\")\n",
        "print(f\"Найдено изображений: {len(imgs)}\")\n",
        "print(f\"Найдено пар изображение-маска: {len(pairs)}\")\n",
        "if len(pairs) == 0:\n",
        "    raise RuntimeError(\"Не найдено ни одной пары изображение-маска! Проверьте имена файлов и структуру папок.\")\n",
        "\n",
        "imgs, masks = zip(*pairs)\n",
        "\n",
        "idxs = np.random.permutation(len(imgs))\n",
        "n = len(imgs)\n",
        "n_train = int(0.8 * n)\n",
        "n_val = int(0.1 * n)\n",
        "split_files = {\n",
        "    \"train\": (np.array(imgs)[idxs[:n_train]], np.array(masks)[idxs[:n_train]]),\n",
        "    \"val\":   (np.array(imgs)[idxs[n_train:n_train+n_val]], np.array(masks)[idxs[n_train:n_train+n_val]]),\n",
        "    \"test\":  (np.array(imgs)[idxs[n_train+n_val:]], np.array(masks)[idxs[n_train+n_val:]])\n",
        "}\n",
        "\n",
        "# ================================================================\n",
        "# 3. АУГМЕНТАЦИИ (ещё сильнее!)\n",
        "# ================================================================\n",
        "def make_resize(h, w):\n",
        "    return A.Resize(height=h, width=w)\n",
        "\n",
        "def make_rrc(h, w, scale=(0.5, 1.0), p=1.0):\n",
        "    return A.RandomResizedCrop(size=(h, w), scale=scale, p=p)\n",
        "\n",
        "train_tf_improved = A.Compose([\n",
        "    make_rrc(H, W, scale=(0.5, 1.0), p=1.0),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=20, p=0.8),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=0.7),\n",
        "    A.CLAHE(p=0.3),\n",
        "    A.RandomGamma(p=0.3),\n",
        "    A.GaussianBlur(blur_limit=(3, 7), p=0.4),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.4),\n",
        "    A.GridDistortion(p=0.2),\n",
        "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.3),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "test_tf_improved = A.Compose([\n",
        "    make_resize(H, W),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# ================================================================\n",
        "# 4. КЛАСС ДАТАСЕТА\n",
        "# ================================================================\n",
        "class DroneBinDS(Dataset):\n",
        "    def __init__(self, imgs_paths, masks_paths, transform_fn):\n",
        "        self.imgs_paths = list(imgs_paths)\n",
        "        self.masks_paths = list(masks_paths)\n",
        "        self.transform_fn = transform_fn\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_paths)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img = cv2.cvtColor(cv2.imread(str(self.imgs_paths[i])), cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(str(self.masks_paths[i]), 0)\n",
        "        mask = (mask > 127).astype(\"float32\")\n",
        "        augmented = self.transform_fn(image=img, mask=mask)\n",
        "        image_tensor = augmented[\"image\"]\n",
        "        mask_tensor = augmented[\"mask\"]\n",
        "        if mask_tensor.ndim == 2:\n",
        "            mask_tensor = mask_tensor.unsqueeze(0)\n",
        "        return image_tensor, mask_tensor.float()\n",
        "\n",
        "# ================================================================\n",
        "# 5. МЕТРИКИ\n",
        "# ================================================================\n",
        "def iou_score(pred, target, threshold=0.5, eps=1e-6):\n",
        "    pred = (pred > threshold).float()\n",
        "    target = (target > threshold).float()\n",
        "    intersection = (pred * target).sum(dim=(1,2,3))\n",
        "    union = (pred + target - pred * target).sum(dim=(1,2,3))\n",
        "    iou = (intersection + eps) / (union + eps)\n",
        "    return iou.mean().item()\n",
        "\n",
        "def dice_score(pred, target, threshold=0.5, eps=1e-6):\n",
        "    pred = (pred > threshold).float()\n",
        "    target = (target > threshold).float()\n",
        "    intersection = (pred * target).sum(dim=(1,2,3))\n",
        "    dice = (2 * intersection + eps) / (pred.sum(dim=(1,2,3)) + target.sum(dim=(1,2,3)) + eps)\n",
        "    return dice.mean().item()\n",
        "\n",
        "def pixel_accuracy(pred, target, threshold=0.5):\n",
        "    pred = (pred > threshold).float()\n",
        "    target = (target > threshold).float()\n",
        "    correct = (pred == target).float().sum()\n",
        "    total = torch.numel(pred)\n",
        "    return (correct / total).item()\n",
        "\n",
        "# ================================================================\n",
        "# 6. U-NET: САМОСТОЯТЕЛЬНАЯ РЕАЛИЗАЦИЯ\n",
        "# ================================================================\n",
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=1, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "        for feat in features:\n",
        "            self.downs.append(UNetBlock(in_ch, feat))\n",
        "            in_ch = feat\n",
        "        self.bottleneck = UNetBlock(features[-1], features[-1]*2)\n",
        "        for feat in reversed(features):\n",
        "            self.ups.append(nn.ConvTranspose2d(feat*2, feat, 2, stride=2))\n",
        "            self.ups.append(UNetBlock(feat*2, feat))\n",
        "        self.final = nn.Conv2d(features[0], out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skips = []\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skips.append(x)\n",
        "            x = F.max_pool2d(x, 2)\n",
        "        x = self.bottleneck(x)\n",
        "        skips = skips[::-1]\n",
        "        for i in range(0, len(self.ups), 2):\n",
        "            x = self.ups[i](x)\n",
        "            skip = skips[i//2]\n",
        "            if x.shape != skip.shape:\n",
        "                x = F.interpolate(x, size=skip.shape[2:])\n",
        "            x = torch.cat([skip, x], dim=1)\n",
        "            x = self.ups[i+1](x)\n",
        "        return self.final(x)\n",
        "\n",
        "# ================================================================\n",
        "# 7. КОМБИНИРОВАННЫЙ ЛОСС (Dice + BCE)\n",
        "# ================================================================\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "    def forward(self, logits, targets):\n",
        "        probs = torch.sigmoid(logits)\n",
        "        targets = (targets > 0.5).float()\n",
        "        intersection = (probs * targets).sum(dim=(1,2,3))\n",
        "        union = probs.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3))\n",
        "        dice = (2 * intersection + self.eps) / (union + self.eps)\n",
        "        return 1 - dice.mean()\n",
        "\n",
        "dice_loss = DiceLoss()\n",
        "bce_loss = nn.BCEWithLogitsLoss()\n",
        "def combo_loss_fn(logits, targets):\n",
        "    return 0.7 * dice_loss(logits, targets) + 0.3 * bce_loss(logits, targets)\n",
        "\n",
        "# ================================================================\n",
        "# 8. PolyLR SCHEDULER\n",
        "# ================================================================\n",
        "def poly_lr_scheduler(optimizer, init_lr, curr_iter, max_iter, power=0.9):\n",
        "    lr = init_lr * (1 - curr_iter / max_iter) ** power\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "# ================================================================\n",
        "# 9. ОБУЧАЮЩИЙ И ВАЛИДАЦИОННЫЙ ЦИКЛ (AMP + PolyLR)\n",
        "# ================================================================\n",
        "def train_one_epoch(model, loader, optimizer, loss_fn, device, scaler, epoch, max_iter, init_lr):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (imgs, masks) in enumerate(tqdm(loader, desc=\"Train\", leave=False)):\n",
        "        imgs, masks = imgs.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model(imgs)\n",
        "            loss = loss_fn(logits, masks)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # PolyLR step\n",
        "        poly_lr_scheduler(optimizer, init_lr, epoch * len(loader) + batch_idx, max_iter)\n",
        "        total_loss += loss.item()\n",
        "        torch.cuda.empty_cache()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    iou, dice, acc, loss_sum = 0, 0, 0, 0\n",
        "    loss_fn = combo_loss_fn\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in tqdm(loader, desc=\"Val\", leave=False):\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "            logits = model(imgs)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            loss = loss_fn(logits, masks)\n",
        "            iou += iou_score(probs, masks)\n",
        "            dice += dice_score(probs, masks)\n",
        "            acc += pixel_accuracy(probs, masks)\n",
        "            loss_sum += loss.item()\n",
        "    n = len(loader)\n",
        "    return {\n",
        "        \"iou\": iou/n,\n",
        "        \"dice\": dice/n,\n",
        "        \"acc\": acc/n,\n",
        "        \"loss\": loss_sum/n\n",
        "    }\n",
        "\n",
        "# ================================================================\n",
        "# 10. ОБУЧЕНИЕ И ТЕСТИРОВАНИЕ\n",
        "# ================================================================\n",
        "BATCH_SIZE = 2\n",
        "NUM_WORKERS = 2\n",
        "EPOCHS = 20\n",
        "INIT_LR = 1e-3\n",
        "\n",
        "train_ds = DroneBinDS(*split_files[\"train\"], train_tf_improved)\n",
        "val_ds   = DroneBinDS(*split_files[\"val\"],   test_tf_improved)\n",
        "test_ds  = DroneBinDS(*split_files[\"test\"],  test_tf_improved)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet(in_ch=3, out_ch=1).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=INIT_LR, weight_decay=1e-5)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "max_iter = EPOCHS * len(train_loader)\n",
        "best_val_iou = 0\n",
        "best_model_state = None\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    torch.cuda.empty_cache()\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, combo_loss_fn, device, scaler, epoch, max_iter, INIT_LR)\n",
        "    val_metrics = evaluate(model, val_loader, device)\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_metrics['loss']:.4f} | Val IoU: {val_metrics['iou']:.4f} | Val Dice: {val_metrics['dice']:.4f} | Val Acc: {val_metrics['acc']:.4f}\")\n",
        "    if val_metrics['iou'] > best_val_iou:\n",
        "        best_val_iou = val_metrics['iou']\n",
        "        best_model_state = model.state_dict()\n",
        "\n",
        "print(\"\\n=== Оценка на тестовой выборке ===\")\n",
        "model.load_state_dict(best_model_state)\n",
        "test_metrics = evaluate(model, test_loader, device)\n",
        "print(f\"Test Loss: {test_metrics['loss']:.4f} | Test IoU: {test_metrics['iou']:.4f} | Test Dice: {test_metrics['dice']:.4f} | Test Acc: {test_metrics['acc']:.4f}\")"
      ],
      "metadata": {
        "id": "zY9FOEwstsTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69c2759-6494-41de-c348-19b792d6c8ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMG_DIR: /kaggle/input/semantic-segmentation-drone-dataset/binary_dataset/binary_dataset/original_images, MASK_DIR: /kaggle/input/semantic-segmentation-drone-dataset/binary_dataset/binary_dataset/images_semantic\n",
            "Найдено изображений: 400\n",
            "Найдено пар изображение-маска: 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n",
            "<ipython-input-5-8e1048d6b9f9>:63: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.4),\n",
            "<ipython-input-5-8e1048d6b9f9>:65: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n",
            "  A.CoarseDropout(max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.3),\n",
            "<ipython-input-5-8e1048d6b9f9>:263: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|          | 0/160 [00:00<?, ?it/s]<ipython-input-5-8e1048d6b9f9>:210: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6722 | Val Loss: 0.6163 | Val IoU: 0.2034 | Val Dice: 0.3231 | Val Acc: 0.7387\n",
            "\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6499 | Val Loss: 0.6034 | Val IoU: 0.1682 | Val Dice: 0.2707 | Val Acc: 0.7502\n",
            "\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6502 | Val Loss: 0.6053 | Val IoU: 0.2124 | Val Dice: 0.3313 | Val Acc: 0.7716\n",
            "\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6256 | Val Loss: 0.6651 | Val IoU: 0.1983 | Val Dice: 0.3130 | Val Acc: 0.7261\n",
            "\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6263 | Val Loss: 0.5714 | Val IoU: 0.2884 | Val Dice: 0.4283 | Val Acc: 0.7603\n",
            "\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6091 | Val Loss: 0.6749 | Val IoU: 0.3486 | Val Dice: 0.4891 | Val Acc: 0.6786\n",
            "\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6087 | Val Loss: 0.6001 | Val IoU: 0.2830 | Val Dice: 0.4154 | Val Acc: 0.7530\n",
            "\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5960 | Val Loss: 0.5761 | Val IoU: 0.2751 | Val Dice: 0.4116 | Val Acc: 0.7885\n",
            "\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5842 | Val Loss: 0.5937 | Val IoU: 0.3103 | Val Dice: 0.4479 | Val Acc: 0.7452\n",
            "\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5884 | Val Loss: 0.5910 | Val IoU: 0.3364 | Val Dice: 0.4801 | Val Acc: 0.7584\n",
            "\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5706 | Val Loss: 0.5647 | Val IoU: 0.3087 | Val Dice: 0.4470 | Val Acc: 0.7882\n",
            "\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5623 | Val Loss: 0.5962 | Val IoU: 0.2699 | Val Dice: 0.4035 | Val Acc: 0.7756\n",
            "\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5636 | Val Loss: 0.5887 | Val IoU: 0.2791 | Val Dice: 0.4162 | Val Acc: 0.7691\n",
            "\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5594 | Val Loss: 0.5809 | Val IoU: 0.3185 | Val Dice: 0.4609 | Val Acc: 0.7646\n",
            "\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5451 | Val Loss: 0.5849 | Val IoU: 0.2784 | Val Dice: 0.4132 | Val Acc: 0.7781\n",
            "\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5580 | Val Loss: 0.5400 | Val IoU: 0.3545 | Val Dice: 0.5084 | Val Acc: 0.7708\n",
            "\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5450 | Val Loss: 0.5537 | Val IoU: 0.3619 | Val Dice: 0.5062 | Val Acc: 0.7704\n",
            "\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5391 | Val Loss: 0.5610 | Val IoU: 0.3568 | Val Dice: 0.4988 | Val Acc: 0.7700\n",
            "\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5447 | Val Loss: 0.5385 | Val IoU: 0.3436 | Val Dice: 0.4921 | Val Acc: 0.7818\n",
            "\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5295 | Val Loss: 0.5495 | Val IoU: 0.3336 | Val Dice: 0.4775 | Val Acc: 0.7786\n",
            "\n",
            "=== Оценка на тестовой выборке ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.4921 | Test IoU: 0.4142 | Test Dice: 0.5728 | Test Acc: 0.8016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты улучшенного собственного U-Net:\n",
        "\n",
        "* Test IoU (mIoU): 0.414\n",
        "* Test Dice: 0.573\n",
        "* Test Pixel Accuracy: 0.802\n",
        "* Test Loss: 0.492\n",
        "\n",
        "Сравнение с собственной базовой реализацией:\n",
        "\n",
        "Улучшенный бейзлайн показывает заметный прирост по всем метрикам по сравнению с базовой собственной реализацией (где mIoU ≈ 0.366, Dice ≈ 0.514, Acc ≈ 0.83, Loss ≈ 0.369).\n",
        "mIoU вырос примерно на 13%, Dice — на 6%, что говорит о положительном влиянии аугментаций, комбинированного лосса и других улучшений."
      ],
      "metadata": {
        "id": "fNvg4dlPqUma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Выводы"
      ],
      "metadata": {
        "id": "P0B00V3Bq2KQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| Модель/Реализация         | mIoU (Test) | Dice (Test) | Pixel Accuracy (Test) | Loss (Test) | Примечания                |\n",
        "|---------------------------|-------------|-------------|-----------------------|-------------|---------------------------|\n",
        "| **U-Net (self, базовый)** |   0.366     |   0.514     |        0.830          |   0.369     | Самостоятельная реализация, базовая |\n",
        "| **U-Net (self, улучш.)**  |   0.414     |   0.573     |        0.802          |   0.492     | Самостоятельная реализация, улучшенная |\n",
        "| U-Net ResNet34 (SMP)      |   0.756     |   0.863    |        0.933          |   0.131     | Библиотечная, базовая     |\n",
        "| U-Net ResNet50 (SMP)      |   0.732     |   0.851    |        0.925          |   0.202     | Библиотечная, улучшенная  |\n",
        "| DeepLabV3+ mit_b0 (SMP)   |   0.727     |   0.856    |        0.926          |   0.141     | Библиотечная, базовая     |\n",
        "| DeepLabV3+ mit_b2 (SMP)   |   0.757     |   0.872    |        0.928          |   0.176     | Библиотечная, улучшенная  |"
      ],
      "metadata": {
        "id": "HS3Gv24qq5Gc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Библиотечные модели (U-Net ResNet34/50, DeepLabV3+) показывают значительно лучшие результаты по всем основным метрикам (mIoU, Pixel Accuracy, Loss) по сравнению с собственными реализациями U-Net. Это объясняется более сложной архитектурой, использованием предобученных энкодеров и оптимизированными реализациями.\n",
        "\n",
        "* Собственные реализации U-Net уступают по качеству, но демонстрируют работоспособность: mIoU и Dice-коэффициент на тесте показывают, что модель действительно учится выделять объекты, хотя и с меньшей точностью.\n",
        "\n",
        "* Улучшение собственной модели (например, за счет доработки архитектуры или обучения) приводит к росту mIoU и Dice, хотя и не позволяет догнать библиотечные решения."
      ],
      "metadata": {
        "id": "cCQHw1aVf7qw"
      }
    }
  ]
}